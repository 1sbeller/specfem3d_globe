!=====================================================================
!
!          S p e c f e m 3 D  G l o b e  V e r s i o n  4 . 1
!          --------------------------------------------------
!
!          Main authors: Dimitri Komatitsch and Jeroen Tromp
!    Seismological Laboratory, California Institute of Technology, USA
!             and University of Pau / CNRS / INRIA, France
! (c) California Institute of Technology and University of Pau / CNRS / INRIA
!                            August 2008
!
! This program is free software; you can redistribute it and/or modify
! it under the terms of the GNU General Public License as published by
! the Free Software Foundation; either version 2 of the License, or
! (at your option) any later version.
!
! This program is distributed in the hope that it will be useful,
! but WITHOUT ANY WARRANTY; without even the implied warranty of
! MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
! GNU General Public License for more details.
!
! You should have received a copy of the GNU General Public License along
! with this program; if not, write to the Free Software Foundation, Inc.,
! 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
!
!=====================================================================
!
! United States and French Government Sponsorship Acknowledged.

!===================================================================!
!                                                                   !
!  Specfem3D is a 3-D spectral-element solver for the Earth.        !
!  It uses a mesh generated by meshfem3D,                           !
!  which produces a spectral element grid for the Earth.            !
!  This is accomplished based upon a mapping of the face of a cube  !
!  to a portion of the sphere (Ronchi et al., The Cubed Sphere).    !
!  Grid density is decreased by a factor of two                     !
!  three times in the radial direction.                             !
!                                                                   !
!===================================================================!
!
! If you use this code for your own research, please cite some of these articles:
!
! @ARTICLE{KoRiTr02,
! author={D. Komatitsch and J. Ritsema and J. Tromp},
! year=2002,
! title={The Spectral-Element Method, {B}eowulf Computing, and Global Seismology},
! journal={Science},
! volume=298,
! number=5599,
! pages={1737-1742},
! doi={10.1126/science.1076024}}
!
! @ARTICLE{KoTr02a,
! author={D. Komatitsch and J. Tromp},
! year=2002,
! title={Spectral-Element Simulations of Global Seismic Wave Propagation{-I. V}alidation},
! journal={Geophys. J. Int.},
! volume=149,
! number=2,
! pages={390-412},
! doi={10.1046/j.1365-246X.2002.01653.x}}
!
! @ARTICLE{KoTr02b,
! author={D. Komatitsch and J. Tromp},
! year=2002,
! title={Spectral-Element Simulations of Global Seismic Wave Propagation{-II. 3-D} Models, Oceans, Rotation, and Self-Gravitation},
! journal={Geophys. J. Int.},
! volume=150,
! pages={303-318},
! number=1,
! doi={10.1046/j.1365-246X.2002.01716.x}}
!
! @ARTICLE{KoTr99,
! author={D. Komatitsch and J. Tromp},
! year=1999,
! title={Introduction to the spectral-element method for 3-{D} seismic wave propagation},
! journal={Geophys. J. Int.},
! volume=139,
! number=3,
! pages={806-822},
! doi={10.1046/j.1365-246x.1999.00967.x}}
!
! @ARTICLE{KoVi98,
! author={D. Komatitsch and J. P. Vilotte},
! title={The spectral-element method: an efficient tool to simulate the seismic response of 2{D} and 3{D} geological structures},
! journal={Bull. Seismol. Soc. Am.},
! year=1998,
! volume=88,
! number=2,
! pages={368-392}}
!
! If you use the kernel capabilities of the code, please cite
!
! @ARTICLE{LiTr06,
! author={Qinya Liu and Jeroen Tromp},
! title={Finite-frequency kernels based on adjoint methods},
! journal={Bull. Seismol. Soc. Am.},
! year=2006,
! volume=96,
! number=6,
! pages={2383-2397},
! doi={10.1785/0120060041}}
!
! If you use 3-D model S20RTS, please cite
!
! @ARTICLE{RiVa00,
! author={J. Ritsema and H. J. {Van Heijst}},
! year=2000,
! title={Seismic imaging of structural heterogeneity in {E}arth's mantle: Evidence for large-scale mantle flow},
! journal={Science Progress},
! volume=83,
! pages={243-259}}
!
! Reference frame - convention:
! ----------------------------
!
! The code uses the following convention for the reference frame:
!
!  - X axis is East
!  - Y axis is North
!  - Z axis is up
!
! Note that this convention is different from both the Aki-Richards convention
! and the Harvard CMT convention.
!
! Let us recall that the Aki-Richards convention is:
!
!  - X axis is North
!  - Y axis is East
!  - Z axis is down
!
! and that the Harvard CMT convention is:
!
!  - X axis is South
!  - Y axis is East
!  - Z axis is up
!
! To report bugs or suggest improvements to the code, please send an email
! to Jeroen Tromp <jtromp AT princeton.edu> and/or use our online
! bug tracking system at http://www.geodynamics.org/roundup .
!
! Evolution of the code:
! ---------------------
!
! v. 4.1_beta Dimitri Komatitsch, University of Pau, France, August 2008:
!      merged the mesher and the solver, support for diskless supercomputers,
!      converted many arrays from memory heap to stack (using automatic arrays instead of allocatable)
!      to avoid memory fragmentation in the case of very large models
!
! v. 4.0 David Michea and Dimitri Komatitsch, University of Pau, France, February 2008:
!      new doubling brick in the mesh, new perfectly load-balanced mesh,
!      more flexible routines for mesh design, new inflated central cube
!      with optimized shape, far fewer mesh files saved by the mesher,
!      global arrays sorted to speed up the simulation, seismograms can be
!      written by the master
!
! v. 3.6 Many people, many affiliations, September 2006:
!      adjoint and kernel calculations (by Qinya Liu), fixed IASP91 model,
!      added AK135 and 1066a, fixed topography/bathymetry routine,
!      new attenuation routines, faster and better I/Os on very large
!      systems, many small improvements and bug fixes, new "configure"
!      script, new Pyre version, new user's manual etc.
!
! v. 3.5 Dimitri Komatitsch, Brian Savage and Jeroen Tromp, Caltech, July 2004:
!      any size of chunk, 3D attenuation, case of two chunks,
!      more precise topography/bathymetry model, new Par_file structure
!
! v. 3.4 Dimitri Komatitsch and Jeroen Tromp, Caltech, August 2003:
!      merged global and regional codes, no iterations in fluid, better movies
!
! v. 3.3 Dimitri Komatitsch, Caltech, September 2002:
!      flexible mesh doubling in outer core, inlined code, OpenDX support for mesh files
!
! v. 3.2 Jeroen Tromp, Caltech, July 2002:
!      multiple sources and flexible PREM reading
!
! v. 3.1 Dimitri Komatitsch, Caltech, June 2002:
!      vectorized loops in solver and merged central cube
!
! v. 3.0 Dimitri Komatitsch and Jeroen Tromp, Caltech, May 2002:
!      ported to SGI and Compaq DEC Alpha, double precision solver, more general anisotropy
!
! v. 2.3 Dimitri Komatitsch and Jeroen Tromp, Caltech, August 2001:
!      gravity, rotation, oceans and 3-D models
!
! v. 2.2 Dimitri Komatitsch and Jeroen Tromp, Caltech, March 2001:
!      final MPI package
!
! v. 2.0 Dimitri Komatitsch, Harvard, January 2000:
!      MPI code for the globe
!
! v. 1.0 Dimitri Komatitsch, Mexico, June 1999:
!      first MPI code for a chunk
!
! Jeroen Tromp, Harvard, July 1998:
!      first chunk solver using OpenMP on Sun
!
! Dimitri Komatitsch, IPG Paris, December 1996:
!      first 3-D solver for the Connection Machine CM-5 (by Thinking Machines)
!

!! DK DK added this for merged version
!! DK DK stored in single precision for merged version, check if it precise enough (probably yes)
!! DK DK now defined as pointers, in order to be able to deallocate them
!! DK DK see for instance http://www.pcc.qub.ac.uk/tec/courses/f77tof90/stu-notes/f90studentMIF_6.html
!! DK DK Section 5.6 about this
  module dyn_array
!---------------------------------------------------------------------
!  Module containing definitions needed to dynamically allocate the values of an array
!---------------------------------------------------------------------
  include "constants.h"
  real(kind=CUSTOM_REAL), dimension(:,:), allocatable :: &
          xelm_store_crust_mantle,yelm_store_crust_mantle,zelm_store_crust_mantle, &
          xelm_store_outer_core,yelm_store_outer_core,zelm_store_outer_core, &
          xelm_store_inner_core,yelm_store_inner_core,zelm_store_inner_core
  end module dyn_array

  program main_program

  implicit none

! standard include of the MPI library
#ifdef USE_MPI
  include 'mpif.h'
#endif

  include "constants.h"
#ifdef USE_MPI
  include "precision.h"
#endif

!! DK DK for the merged version
! include values created by the mesher
  include "values_from_mesher.h"

! proc numbers for MPI
  integer myrank,sizeprocs,ier

! use integer array to store values
  integer, dimension(NX_BATHY,NY_BATHY) :: ibathy_topo

! addressing for all the slices
  integer, dimension(0:NPROCTOT_VAL-1) :: ichunk_slice,iproc_xi_slice,iproc_eta_slice
  integer, dimension(NCHUNKS_VAL,0:NPROC_XI_VAL-1,0:NPROC_ETA_VAL-1) :: addressing

  integer :: NTSTEP_BETWEEN_OUTPUT_SEISMOS,NSOURCES

  integer, external :: err_occurred

!!!! DK DK for merged version, all the arrays below are allocated statically instead

#ifdef USE_MPI
  integer npoin2D_faces_crust_mantle(NUMFACES_SHARED)
  integer npoin2D_faces_outer_core(NUMFACES_SHARED)
  integer npoin2D_faces_inner_core(NUMFACES_SHARED)
#endif

! for non blocking communications
  logical, dimension(NSPEC_CRUST_MANTLE) :: is_on_a_slice_edge_crust_mantle
  logical, dimension(NSPEC_OUTER_CORE) :: is_on_a_slice_edge_outer_core
  logical, dimension(NSPEC_INNER_CORE) :: is_on_a_slice_edge_inner_core

  integer, dimension(NB_SQUARE_EDGES_ONEDIR) :: npoin2D_xi_crust_mantle,npoin2D_eta_crust_mantle, &
             npoin2D_xi_outer_core,npoin2D_eta_outer_core, &
             npoin2D_xi_inner_core,npoin2D_eta_inner_core

  real(kind=CUSTOM_REAL), dimension(NGLOB_CRUST_MANTLE_OCEANS) :: rmass_ocean_load
  real(kind=CUSTOM_REAL), dimension(NDIM,NGLLX,NGLLY,NSPEC2D_TOP_CM) :: normal_top_crust_mantle
  integer, dimension(NSPEC2D_TOP_CM) :: ibelm_top_crust_mantle

! number of elements on the boundaries
  integer :: nspec2D_xmin_inner_core,nspec2D_xmax_inner_core,nspec2D_ymin_inner_core,nspec2D_ymax_inner_core

  real(kind=CUSTOM_REAL), dimension(NGLOB_CRUST_MANTLE) :: rmass_crust_mantle
  real(kind=CUSTOM_REAL), dimension(NGLOB_OUTER_CORE) :: rmass_outer_core
  real(kind=CUSTOM_REAL), dimension(NGLOB_INNER_CORE) :: rmass_inner_core

  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NGLLZ,NSPECMAX_ISO_MANTLE) :: kappavstore_crust_mantle,muvstore_crust_mantle

  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NGLLZ,NSPECMAX_TISO_MANTLE) :: &
    kappahstore_crust_mantle,muhstore_crust_mantle,eta_anisostore_crust_mantle

  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NGLLZ,NSPEC_INNER_CORE) :: kappavstore_inner_core,muvstore_inner_core

! 2-D jacobians and normals

  integer, dimension(NSPEC2D_BOTTOM_CM) :: ibelm_bottom_crust_mantle

  integer, dimension(NSPEC2D_BOTTOM_OC) :: ibelm_bottom_outer_core
  integer, dimension(NSPEC2D_TOP_OC) :: ibelm_top_outer_core

  real(kind=CUSTOM_REAL), dimension(NDIM,NGLLX,NGLLY,NSPEC2D_BOTTOM_OC) :: normal_bottom_outer_core
  real(kind=CUSTOM_REAL), dimension(NDIM,NGLLX,NGLLY,NSPEC2D_TOP_OC) :: normal_top_outer_core

  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NSPEC2D_BOTTOM_OC) :: jacobian2D_bottom_outer_core
  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NSPEC2D_TOP_OC) :: jacobian2D_top_outer_core

  integer, dimension(NSPEC2DMAX_XMIN_XMAX_IC) :: ibelm_xmin_inner_core,ibelm_xmax_inner_core
  integer, dimension(NSPEC2DMAX_YMIN_YMAX_IC) :: ibelm_ymin_inner_core,ibelm_ymax_inner_core
  integer, dimension(NSPEC2D_BOTTOM_IC) :: ibelm_bottom_inner_core
  integer, dimension(NSPEC2D_TOP_IC) :: ibelm_top_inner_core

#ifdef USE_MPI
  integer, dimension(NGLOB2DMAX_XY_VAL_CM,NUMFACES_SHARED) :: iboolfaces_crust_mantle
  integer, dimension(NGLOB2DMAX_XY_VAL_OC,NUMFACES_SHARED) :: iboolfaces_outer_core
  integer, dimension(NGLOB2DMAX_XY_VAL_IC,NUMFACES_SHARED) :: iboolfaces_inner_core
#endif

  integer, dimension(NGLLX,NGLLY,NGLLZ,NSPEC_CRUST_MANTLE) :: ibool_crust_mantle

  integer, dimension(NSPEC_CRUST_MANTLE) :: idoubling_crust_mantle
  integer, dimension(NSPEC_INNER_CORE) :: idoubling_inner_core

  integer, dimension(NGLLX,NGLLY,NGLLZ,NSPEC_OUTER_CORE) :: ibool_outer_core
  integer, dimension(NGLLX,NGLLY,NGLLZ,NSPEC_INNER_CORE) :: ibool_inner_core

#ifdef USE_MPI
  integer, dimension(NGLOB1D_RADIAL_CM,NUMCORNERS_SHARED) :: iboolcorner_crust_mantle
  integer, dimension(NGLOB1D_RADIAL_OC,NUMCORNERS_SHARED) :: iboolcorner_outer_core
  integer, dimension(NGLOB1D_RADIAL_IC,NUMCORNERS_SHARED) :: iboolcorner_inner_core

  integer, dimension(NGLOB2DMAX_XMIN_XMAX_CM) :: iboolleft_xi_crust_mantle,iboolright_xi_crust_mantle
  integer, dimension(NGLOB2DMAX_YMIN_YMAX_CM) :: iboolleft_eta_crust_mantle,iboolright_eta_crust_mantle
  integer, dimension(NGLOB2DMAX_XMIN_XMAX_OC) :: iboolleft_xi_outer_core,iboolright_xi_outer_core
  integer, dimension(NGLOB2DMAX_YMIN_YMAX_OC) :: iboolleft_eta_outer_core,iboolright_eta_outer_core
  integer, dimension(NGLOB2DMAX_XMIN_XMAX_IC) :: iboolleft_xi_inner_core,iboolright_xi_inner_core
  integer, dimension(NGLOB2DMAX_YMIN_YMAX_IC) :: iboolleft_eta_inner_core,iboolright_eta_inner_core
#endif

  integer :: npoin2D_max_all_CM_IC

! receiver information
  integer :: nrec

!---- arrays to assemble between chunks

#ifdef USE_MPI
! communication pattern for faces between chunks
  integer, dimension(NUMMSGS_FACES_VAL) :: imsg_type,iprocfrom_faces,iprocto_faces

! communication pattern for corners between chunks
  integer, dimension(NCORNERSCHUNKS_VAL) :: iproc_master_corners,iproc_worker1_corners,iproc_worker2_corners
#endif

! attenuation_model_variables
  type attenuation_model_variables
    sequence
    double precision min_period, max_period
    double precision                          :: QT_c_source        ! Source Frequency
    double precision, dimension(N_SLS)        :: Qtau_s             ! tau_sigma
    double precision, dimension(:), pointer   :: QrDisc             ! Discontinutitues Defined
    double precision, dimension(:), pointer   :: Qr                 ! Radius
    integer, dimension(:), pointer            :: Qs                 ! Steps
    double precision, dimension(:), pointer   :: Qmu                ! Shear Attenuation
    double precision, dimension(:,:), pointer :: Qtau_e             ! tau_epsilon
    double precision, dimension(:), pointer   :: Qone_minus_sum_beta, Qone_minus_sum_beta2      ! one_minus_sum_beta
    double precision, dimension(:,:), pointer :: Qfc, Qfc2          ! factor_common
    double precision, dimension(:), pointer   :: Qsf, Qsf2          ! scale_factor
    integer, dimension(:), pointer            :: Qrmin              ! Max and Mins of idoubling
    integer, dimension(:), pointer            :: Qrmax              ! Max and Mins of idoubling
    integer                                   :: Qn                 ! Number of points
  end type attenuation_model_variables

  type (attenuation_model_variables) AM_V
! attenuation_model_variables

! arrays from BCAST
  integer, dimension(NVALUES_bcast_integer) :: bcast_integer
  double precision, dimension(NVALUES_bcast_double_precision) :: bcast_double_precision
  logical, dimension(NVALUES_bcast_logical) :: bcast_logical

  character(len=150) MODEL

! computed in read_compute_parameters
  integer, dimension(MAX_NUMBER_OF_MESH_LAYERS) :: ner,ratio_sampling_array
  integer, dimension(MAX_NUMBER_OF_MESH_LAYERS) :: doubling_index
  double precision, dimension(MAX_NUMBER_OF_MESH_LAYERS) :: r_bottom,r_top
  logical, dimension(MAX_NUMBER_OF_MESH_LAYERS) :: this_layer_has_a_doubling
  double precision, dimension(MAX_NUMBER_OF_MESH_LAYERS) :: rmins,rmaxs

! this for all the regions
  integer, dimension(MAX_NUM_REGIONS) :: NSPEC_computed, &
               NSPEC2D_XI, NSPEC2D_ETA, &
               NSPEC2DMAX_XMIN_XMAX,NSPEC2DMAX_YMIN_YMAX, &
               NSPEC2D_BOTTOM,NSPEC2D_TOP, &
               NSPEC1D_RADIAL,NGLOB1D_RADIAL, &
               NGLOB2DMAX_XMIN_XMAX,NGLOB2DMAX_YMIN_YMAX, &
               NGLOB_computed

  integer, dimension(NB_SQUARE_CORNERS,NB_CUT_CASE) :: DIFF_NSPEC1D_RADIAL
  integer, dimension(NB_SQUARE_EDGES_ONEDIR,NB_CUT_CASE) :: DIFF_NSPEC2D_XI,DIFF_NSPEC2D_ETA

! use equivalence statements to reduce total memory size
  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NGLLZ,NSPEC_CRUST_MANTLE) :: &
        xix_crust_mantle,xiy_crust_mantle,xiz_crust_mantle,&
        etax_crust_mantle,etay_crust_mantle,etaz_crust_mantle, &
        gammax_crust_mantle,gammay_crust_mantle,gammaz_crust_mantle

  integer, dimension(NSPEC_CRUST_MANTLE * NGLLX * NGLLY * NGLLZ) :: locval
  logical, dimension(NSPEC_CRUST_MANTLE * NGLLX * NGLLY * NGLLZ) :: ifseg

  integer, dimension(NGLLX,NGLLY,NGLLZ,NSPEC_CRUST_MANTLE) :: copy_ibool_ori

  integer, dimension(NGLOB_CRUST_MANTLE) :: mask_ibool

  double precision, dimension(NGLLX,NGLLY,NGLLZ,NSPEC_CRUST_MANTLE) :: xstore,ystore,zstore

! displacement, velocity, acceleration
  real(kind=CUSTOM_REAL), dimension(NDIM,NGLOB_CRUST_MANTLE) :: displ_crust_mantle

  equivalence(locval,         xix_crust_mantle)
  equivalence(ifseg,          xiy_crust_mantle)
  equivalence(copy_ibool_ori, xiz_crust_mantle)

  equivalence(xstore,         etax_crust_mantle)
  equivalence(ystore,         etay_crust_mantle)
  equivalence(zstore,         etaz_crust_mantle)

  equivalence(mask_ibool,     displ_crust_mantle)

! because NSPEC_CRUST_MANTLE is always an even number, we can put
! two single-precision arrays in each double-precision array.
! this does *NOT* work if double precision is turned on because
! the size of an integer does not correspond to the size of a double.
! but if one comments out the three lines below the other equivalence statements
! above will work fine even in double precision.
  equivalence(xstore(1,1,1,NSPEC_CRUST_MANTLE/2+1), gammax_crust_mantle)
  equivalence(ystore(1,1,1,NSPEC_CRUST_MANTLE/2+1), gammay_crust_mantle)
  equivalence(zstore(1,1,1,NSPEC_CRUST_MANTLE/2+1), gammaz_crust_mantle)

! ************** PROGRAM STARTS HERE **************

! initialize the MPI communicator and start the NPROCTOT MPI processes.
#ifdef USE_MPI
  call MPI_INIT(ier)
#else
  ier = 0
#endif
  if(ier /= 0) stop 'error: cannot start MPI!!!'

! sizeprocs returns number of processes started (should be equal to NPROCTOT).
! myrank is the rank of each process, between 0 and NPROCTOT-1.
! as usual in MPI, process 0 is in charge of coordinating everything
! and also takes care of the main output
! do not create anything for the inner core here, will be done in solver
#ifdef USE_MPI
  call MPI_COMM_SIZE(MPI_COMM_WORLD,sizeprocs,ier)
  call MPI_COMM_RANK(MPI_COMM_WORLD,myrank,ier)
#else
  myrank = 0
  sizeprocs = NPROCTOT_VAL
#endif

  if(CUSTOM_REAL /= 4) &
    stop 'some of the equivalence statements used to save memory do not work in double precision, please edit and recompile'

! make sure there is no overlap between arrays due to the "equivalence" statement
  etax_crust_mantle = -10._CUSTOM_REAL
  gammax_crust_mantle = +20._CUSTOM_REAL
  if(maxval(abs(etax_crust_mantle + 10._CUSTOM_REAL)) > 0.001_CUSTOM_REAL .or. &
     maxval(abs(gammax_crust_mantle - 20._CUSTOM_REAL)) > 0.001_CUSTOM_REAL) &
       stop 'error: memory overlap between arrays in equivalence statement'

  gammax_crust_mantle = +20._CUSTOM_REAL
  etax_crust_mantle = -10._CUSTOM_REAL
  if(maxval(abs(etax_crust_mantle + 10._CUSTOM_REAL)) > 0.001_CUSTOM_REAL .or. &
     maxval(abs(gammax_crust_mantle - 20._CUSTOM_REAL)) > 0.001_CUSTOM_REAL) &
       stop 'error: memory overlap between arrays in equivalence statement'

! YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY

!! DK DK for the merged version, mesher inserted here
  call meshfem3D(myrank,sizeprocs,addressing,ichunk_slice,iproc_xi_slice,iproc_eta_slice,ibathy_topo,NSOURCES, &
  NTSTEP_BETWEEN_OUTPUT_SEISMOS,ibool_crust_mantle,ibool_outer_core,ibool_inner_core, &
  idoubling_crust_mantle,idoubling_inner_core,ibelm_bottom_crust_mantle, ibelm_bottom_outer_core, ibelm_top_outer_core, &
  ibelm_xmin_inner_core,ibelm_xmax_inner_core,ibelm_ymin_inner_core,ibelm_ymax_inner_core,ibelm_bottom_inner_core, &
  ibelm_top_inner_core,jacobian2D_bottom_outer_core,jacobian2D_top_outer_core, &
  normal_bottom_outer_core, normal_top_outer_core,kappavstore_crust_mantle,muvstore_crust_mantle, &
  kappahstore_crust_mantle,muhstore_crust_mantle,eta_anisostore_crust_mantle,kappavstore_inner_core,muvstore_inner_core, &
  rmass_crust_mantle, rmass_outer_core, rmass_inner_core, &
  nspec2D_xmin_inner_core,nspec2D_xmax_inner_core,nspec2D_ymin_inner_core,nspec2D_ymax_inner_core, &
#ifdef USE_MPI
  iboolleft_xi_crust_mantle,iboolright_xi_crust_mantle, iboolleft_eta_crust_mantle,iboolright_eta_crust_mantle, &
  iboolleft_xi_outer_core,iboolright_xi_outer_core,iboolleft_eta_outer_core,iboolright_eta_outer_core, &
  iboolleft_xi_inner_core,iboolright_xi_inner_core, iboolleft_eta_inner_core,iboolright_eta_inner_core,&
  iprocfrom_faces,iprocto_faces,imsg_type,iproc_master_corners,iproc_worker1_corners,iproc_worker2_corners, &
  iboolfaces_crust_mantle,iboolfaces_outer_core,iboolfaces_inner_core, &
  iboolcorner_crust_mantle,iboolcorner_outer_core,iboolcorner_inner_core, &
  npoin2D_faces_crust_mantle,npoin2D_xi_crust_mantle,npoin2D_eta_crust_mantle, &
  npoin2D_faces_outer_core,npoin2D_xi_outer_core,npoin2D_eta_outer_core, &
  npoin2D_faces_inner_core,npoin2D_xi_inner_core,npoin2D_eta_inner_core, &
#endif
  rmass_ocean_load,normal_top_crust_mantle,ibelm_top_crust_mantle,AM_V, &
  locval,ifseg,copy_ibool_ori,mask_ibool,xstore,ystore,zstore,nrec, &
  bcast_integer,bcast_double_precision,bcast_logical,MODEL,ner,ratio_sampling_array,doubling_index, &
  r_bottom,r_top,rmins,rmaxs,this_layer_has_a_doubling,NSPEC_computed,NSPEC2D_XI,NSPEC2D_ETA, &
  NSPEC2DMAX_XMIN_XMAX,NSPEC2DMAX_YMIN_YMAX,NSPEC2D_BOTTOM,NSPEC2D_TOP,NSPEC1D_RADIAL,NGLOB1D_RADIAL, &
  NGLOB2DMAX_XMIN_XMAX,NGLOB2DMAX_YMIN_YMAX,NGLOB_computed,DIFF_NSPEC1D_RADIAL,DIFF_NSPEC2D_ETA,DIFF_NSPEC2D_XI, &
  is_on_a_slice_edge_crust_mantle,is_on_a_slice_edge_outer_core,is_on_a_slice_edge_inner_core)

! synchronize all the processes to make sure everybody has finished creating the mesh
#ifdef USE_MPI
  call MPI_BARRIER(MPI_COMM_WORLD,ier)
#endif

! YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY

  if(.not. MESHER_ONLY) then

! added this to reduce the size of the buffers
! size of buffers is the sum of two sizes because we handle two regions in the same MPI call
  npoin2D_max_all_CM_IC = max(maxval(npoin2D_xi_crust_mantle(:) + npoin2D_xi_inner_core(:)), &
                        maxval(npoin2D_eta_crust_mantle(:) + npoin2D_eta_inner_core(:)))

! fix the non-blocking arrays to assemble inside the chunks: elements
! in contact with the MPI faces by an edge or a corner only but not
! a full face are missing, therefore let us add them
#ifdef USE_MPI
  call fix_non_blocking_slices(is_on_a_slice_edge_crust_mantle,iboolright_xi_crust_mantle, &
         iboolleft_xi_crust_mantle,iboolright_eta_crust_mantle,iboolleft_eta_crust_mantle, &
         npoin2D_xi_crust_mantle,npoin2D_eta_crust_mantle,ibool_crust_mantle, &
         mask_ibool,NSPEC_CRUST_MANTLE,NGLOB_CRUST_MANTLE,NGLOB2DMAX_XMIN_XMAX_CM,NGLOB2DMAX_YMIN_YMAX_CM)

  call fix_non_blocking_slices(is_on_a_slice_edge_outer_core,iboolright_xi_outer_core, &
         iboolleft_xi_outer_core,iboolright_eta_outer_core,iboolleft_eta_outer_core, &
         npoin2D_xi_outer_core,npoin2D_eta_outer_core,ibool_outer_core, &
         mask_ibool,NSPEC_OUTER_CORE,NGLOB_OUTER_CORE,NGLOB2DMAX_XMIN_XMAX_OC,NGLOB2DMAX_YMIN_YMAX_OC)

  call fix_non_blocking_slices(is_on_a_slice_edge_inner_core,iboolright_xi_inner_core, &
         iboolleft_xi_inner_core,iboolright_eta_inner_core,iboolleft_eta_inner_core, &
         npoin2D_xi_inner_core,npoin2D_eta_inner_core,ibool_inner_core, &
         mask_ibool,NSPEC_INNER_CORE,NGLOB_INNER_CORE,NGLOB2DMAX_XMIN_XMAX_IC,NGLOB2DMAX_YMIN_YMAX_IC)
#endif

!! DK DK for the merged version, solver inserted here
  call specfem3D(myrank,sizeprocs,ichunk_slice,iproc_xi_slice,iproc_eta_slice,NSOURCES, &
  NTSTEP_BETWEEN_OUTPUT_SEISMOS,ibool_crust_mantle,ibool_outer_core,ibool_inner_core, &
  idoubling_crust_mantle,idoubling_inner_core,ibelm_bottom_crust_mantle, ibelm_bottom_outer_core, ibelm_top_outer_core, &
ibelm_top_inner_core,jacobian2D_bottom_outer_core,jacobian2D_top_outer_core, &
  normal_bottom_outer_core, normal_top_outer_core,kappavstore_crust_mantle,muvstore_crust_mantle, &
  kappahstore_crust_mantle,muhstore_crust_mantle,eta_anisostore_crust_mantle,kappavstore_inner_core,muvstore_inner_core, &
  rmass_crust_mantle,rmass_outer_core,rmass_inner_core,rmass_ocean_load, &
#ifdef USE_MPI
  npoin2D_max_all_CM_IC,nrec,addressing,ibathy_topo, &
  ibelm_xmin_inner_core,ibelm_xmax_inner_core,ibelm_ymin_inner_core,ibelm_ymax_inner_core,ibelm_bottom_inner_core, &
iboolleft_xi_crust_mantle,iboolright_xi_crust_mantle, iboolleft_eta_crust_mantle,iboolright_eta_crust_mantle, &
iboolleft_xi_outer_core,iboolright_xi_outer_core,iboolleft_eta_outer_core,iboolright_eta_outer_core, &
  iboolleft_xi_inner_core,iboolright_xi_inner_core, iboolleft_eta_inner_core,iboolright_eta_inner_core,&
  nspec2D_xmin_inner_core,nspec2D_xmax_inner_core,nspec2D_ymin_inner_core,nspec2D_ymax_inner_core, &
iprocfrom_faces,iprocto_faces,imsg_type,iproc_master_corners,iproc_worker1_corners,iproc_worker2_corners, &
  iboolfaces_crust_mantle,iboolfaces_outer_core,iboolfaces_inner_core, &
  iboolcorner_crust_mantle,iboolcorner_outer_core,iboolcorner_inner_core, &
  npoin2D_faces_crust_mantle,npoin2D_xi_crust_mantle,npoin2D_eta_crust_mantle, &
  npoin2D_faces_outer_core,npoin2D_xi_outer_core,npoin2D_eta_outer_core, &
  npoin2D_faces_inner_core,npoin2D_xi_inner_core,npoin2D_eta_inner_core, &
  normal_top_crust_mantle,ibelm_top_crust_mantle, &
#endif
  AM_V,xix_crust_mantle,xiy_crust_mantle,xiz_crust_mantle,&
  etax_crust_mantle,etay_crust_mantle,etaz_crust_mantle, &
  gammax_crust_mantle,gammay_crust_mantle,gammaz_crust_mantle,displ_crust_mantle, &
  bcast_integer,bcast_double_precision,bcast_logical,MODEL,ner,ratio_sampling_array,doubling_index, &
  r_bottom,r_top,rmins,rmaxs,this_layer_has_a_doubling,NSPEC_computed,NSPEC2D_XI,NSPEC2D_ETA, &
  NSPEC2DMAX_XMIN_XMAX,NSPEC2DMAX_YMIN_YMAX,NSPEC2D_BOTTOM,NSPEC2D_TOP,NSPEC1D_RADIAL,NGLOB1D_RADIAL, &
  NGLOB2DMAX_XMIN_XMAX,NGLOB2DMAX_YMIN_YMAX,NGLOB_computed,DIFF_NSPEC1D_RADIAL,DIFF_NSPEC2D_ETA,DIFF_NSPEC2D_XI, &
  is_on_a_slice_edge_crust_mantle,is_on_a_slice_edge_outer_core,is_on_a_slice_edge_inner_core)

! synchronize all the processes to make sure everybody has finished
#ifdef USE_MPI
  call MPI_BARRIER(MPI_COMM_WORLD,ier)
#endif

  endif ! end of test if running the mesher only but not the solver

! stop all the MPI processes, and exit
#ifdef USE_MPI
  call MPI_FINALIZE(ier)
#endif

  end program main_program

