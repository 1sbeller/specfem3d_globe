!=====================================================================
!
!          S p e c f e m 3 D  G l o b e  V e r s i o n  4 . 1
!          --------------------------------------------------
!
!          Main authors: Dimitri Komatitsch and Jeroen Tromp
!    Seismological Laboratory, California Institute of Technology, USA
!             and University of Pau / CNRS / INRIA, France
! (c) California Institute of Technology and University of Pau / CNRS / INRIA
!                            August 2008
!
! This program is free software; you can redistribute it and/or modify
! it under the terms of the GNU General Public License as published by
! the Free Software Foundation; either version 2 of the License, or
! (at your option) any later version.
!
! This program is distributed in the hope that it will be useful,
! but WITHOUT ANY WARRANTY; without even the implied warranty of
! MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
! GNU General Public License for more details.
!
! You should have received a copy of the GNU General Public License along
! with this program; if not, write to the Free Software Foundation, Inc.,
! 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
!
!=====================================================================
!
! United States and French Government Sponsorship Acknowledged.

!===================================================================!
!                                                                   !
!  Specfem3D is a 3-D spectral-element solver for the Earth.        !
!  It uses a mesh generated by meshfem3D,                           !
!  which produces a spectral element grid for the Earth.            !
!  This is accomplished based upon a mapping of the face of a cube  !
!  to a portion of the sphere (Ronchi et al., The Cubed Sphere).    !
!  Grid density is decreased by a factor of two                     !
!  three times in the radial direction.                             !
!                                                                   !
!===================================================================!
!
! If you use this code for your own research, please cite some of these articles:
!
! @ARTICLE{KoRiTr02,
! author={D. Komatitsch and J. Ritsema and J. Tromp},
! year=2002,
! title={The Spectral-Element Method, {B}eowulf Computing, and Global Seismology},
! journal={Science},
! volume=298,
! number=5599,
! pages={1737-1742},
! doi={10.1126/science.1076024}}
!
! @ARTICLE{KoTr02a,
! author={D. Komatitsch and J. Tromp},
! year=2002,
! title={Spectral-Element Simulations of Global Seismic Wave Propagation{-I. V}alidation},
! journal={Geophys. J. Int.},
! volume=149,
! number=2,
! pages={390-412},
! doi={10.1046/j.1365-246X.2002.01653.x}}
!
! @ARTICLE{KoTr02b,
! author={D. Komatitsch and J. Tromp},
! year=2002,
! title={Spectral-Element Simulations of Global Seismic Wave Propagation{-II. 3-D} Models, Oceans, Rotation, and Self-Gravitation},
! journal={Geophys. J. Int.},
! volume=150,
! pages={303-318},
! number=1,
! doi={10.1046/j.1365-246X.2002.01716.x}}
!
! @ARTICLE{KoTr99,
! author={D. Komatitsch and J. Tromp},
! year=1999,
! title={Introduction to the spectral-element method for 3-{D} seismic wave propagation},
! journal={Geophys. J. Int.},
! volume=139,
! number=3,
! pages={806-822},
! doi={10.1046/j.1365-246x.1999.00967.x}}
!
! @ARTICLE{KoVi98,
! author={D. Komatitsch and J. P. Vilotte},
! title={The spectral-element method: an efficient tool to simulate the seismic response of 2{D} and 3{D} geological structures},
! journal={Bull. Seismol. Soc. Am.},
! year=1998,
! volume=88,
! number=2,
! pages={368-392}}
!
! If you use the kernel capabilities of the code, please cite
!
! @ARTICLE{LiTr06,
! author={Qinya Liu and Jeroen Tromp},
! title={Finite-frequency kernels based on adjoint methods},
! journal={Bull. Seismol. Soc. Am.},
! year=2006,
! volume=96,
! number=6,
! pages={2383-2397},
! doi={10.1785/0120060041}}
!
! If you use 3-D model S20RTS, please cite
!
! @ARTICLE{RiVa00,
! author={J. Ritsema and H. J. {Van Heijst}},
! year=2000,
! title={Seismic imaging of structural heterogeneity in {E}arth's mantle: Evidence for large-scale mantle flow},
! journal={Science Progress},
! volume=83,
! pages={243-259}}
!
! Reference frame - convention:
! ----------------------------
!
! The code uses the following convention for the reference frame:
!
!  - X axis is East
!  - Y axis is North
!  - Z axis is up
!
! Note that this convention is different from both the Aki-Richards convention
! and the Harvard CMT convention.
!
! Let us recall that the Aki-Richards convention is:
!
!  - X axis is North
!  - Y axis is East
!  - Z axis is down
!
! and that the Harvard CMT convention is:
!
!  - X axis is South
!  - Y axis is East
!  - Z axis is up
!
! To report bugs or suggest improvements to the code, please send an email
! to Jeroen Tromp <jtromp AT princeton.edu> and/or use our online
! bug tracking system at http://www.geodynamics.org/roundup .
!
! Evolution of the code:
! ---------------------
!
! v. 4.1_beta Dimitri Komatitsch, University of Pau, France, August 2008:
!      merged the mesher and the solver, support for diskless supercomputers,
!      converted many arrays from memory heap to stack (using automatic arrays instead of allocatable)
!      to avoid memory fragmentation in the case of very large models
!
! v. 4.0 David Michea and Dimitri Komatitsch, University of Pau, France, February 2008:
!      new doubling brick in the mesh, new perfectly load-balanced mesh,
!      more flexible routines for mesh design, new inflated central cube
!      with optimized shape, far fewer mesh files saved by the mesher,
!      global arrays sorted to speed up the simulation, seismograms can be
!      written by the master
!
! v. 3.6 Many people, many affiliations, September 2006:
!      adjoint and kernel calculations (by Qinya Liu), fixed IASP91 model,
!      added AK135 and 1066a, fixed topography/bathymetry routine,
!      new attenuation routines, faster and better I/Os on very large
!      systems, many small improvements and bug fixes, new "configure"
!      script, new Pyre version, new user's manual etc.
!
! v. 3.5 Dimitri Komatitsch, Brian Savage and Jeroen Tromp, Caltech, July 2004:
!      any size of chunk, 3D attenuation, case of two chunks,
!      more precise topography/bathymetry model, new Par_file structure
!
! v. 3.4 Dimitri Komatitsch and Jeroen Tromp, Caltech, August 2003:
!      merged global and regional codes, no iterations in fluid, better movies
!
! v. 3.3 Dimitri Komatitsch, Caltech, September 2002:
!      flexible mesh doubling in outer core, inlined code, OpenDX support for mesh files
!
! v. 3.2 Jeroen Tromp, Caltech, July 2002:
!      multiple sources and flexible PREM reading
!
! v. 3.1 Dimitri Komatitsch, Caltech, June 2002:
!      vectorized loops in solver and merged central cube
!
! v. 3.0 Dimitri Komatitsch and Jeroen Tromp, Caltech, May 2002:
!      ported to SGI and Compaq DEC Alpha, double precision solver, more general anisotropy
!
! v. 2.3 Dimitri Komatitsch and Jeroen Tromp, Caltech, August 2001:
!      gravity, rotation, oceans and 3-D models
!
! v. 2.2 Dimitri Komatitsch and Jeroen Tromp, Caltech, March 2001:
!      final MPI package
!
! v. 2.0 Dimitri Komatitsch, Harvard, January 2000:
!      MPI code for the globe
!
! v. 1.0 Dimitri Komatitsch, Mexico, June 1999:
!      first MPI code for a chunk
!
! Jeroen Tromp, Harvard, July 1998:
!      first chunk solver using OpenMP on Sun
!
! Dimitri Komatitsch, IPG Paris, December 1996:
!      first 3-D solver for the Connection Machine CM-5 (by Thinking Machines)
!

!! DK DK added this for merged version
!! DK DK stored in single precision for merged version, check if it precise enough (probably yes)
!! DK DK now defined as pointers, in order to be able to deallocate them
!! DK DK see for instance http://www.pcc.qub.ac.uk/tec/courses/f77tof90/stu-notes/f90studentMIF_6.html
!! DK DK Section 5.6 about this
  module dyn_array
!---------------------------------------------------------------------
!  Module containing definitions needed to dynamically allocate the values of an array 
!---------------------------------------------------------------------
  include "constants.h"
  real(kind=CUSTOM_REAL), dimension(:,:), allocatable :: &
          xelm_store_crust_mantle,yelm_store_crust_mantle,zelm_store_crust_mantle, &
          xelm_store_outer_core,yelm_store_outer_core,zelm_store_outer_core, &
          xelm_store_inner_core,yelm_store_inner_core,zelm_store_inner_core
  end module dyn_array

  program main_program

  implicit none

! standard include of the MPI library
  include 'mpif.h'

  include "constants.h"
  include "precision.h"

!! DK DK for the merged version
! include values created by the mesher
  include "values_from_mesher.h"

! proc numbers for MPI
  integer myrank,sizeprocs,ier

! use integer array to store values
  integer, dimension(NX_BATHY,NY_BATHY) :: ibathy_topo

! addressing for all the slices
  integer, dimension(0:NPROCTOT_VAL-1) :: ichunk_slice,iproc_xi_slice,iproc_eta_slice
  integer, dimension(NCHUNKS_VAL,0:NPROC_XI_VAL-1,0:NPROC_ETA_VAL-1) :: addressing

  integer :: NTSTEP_BETWEEN_OUTPUT_SEISMOS,NSOURCES

  integer, external :: err_occurred

!! DK DK for the merged version
  include 'declarations_main.f90'

! ************** PROGRAM STARTS HERE **************

! initialize the MPI communicator and start the NPROCTOT MPI processes.
  call MPI_INIT(ier)
  if(ier /= 0) stop 'error: cannot start MPI!!!'

! sizeprocs returns number of processes started (should be equal to NPROCTOT).
! myrank is the rank of each process, between 0 and NPROCTOT-1.
! as usual in MPI, process 0 is in charge of coordinating everything
! and also takes care of the main output
! do not create anything for the inner core here, will be done in solver
  call MPI_COMM_SIZE(MPI_COMM_WORLD,sizeprocs,ier)
  call MPI_COMM_RANK(MPI_COMM_WORLD,myrank,ier)

! YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY

!! DK DK for the merged version
!!!!!!!! DK DK mesher inserted here
!!!!!!!! DK DK mesher inserted here
!!!!!!!! DK DK mesher inserted here
  include 'call_meshfem1.f90'

! synchronize all the processes to make sure everybody has finished
  call MPI_BARRIER(MPI_COMM_WORLD,ier)

! YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY

!! DK DK for merged version, temporary patch for David's code to cut the superbrick
!! DK DK which I have not fully ported to the merged version yet: I do not
!! DK DK yet distinguish the two values of each array, therefore let me set them
!! DK DK equal here
  npoin2D_xi_crust_mantle(2) = npoin2D_xi_crust_mantle(1)
  npoin2D_eta_crust_mantle(2) = npoin2D_eta_crust_mantle(1)

  npoin2D_xi_outer_core(2) = npoin2D_xi_outer_core(1)
  npoin2D_eta_outer_core(2) = npoin2D_eta_outer_core(1)

  npoin2D_xi_inner_core(2) = npoin2D_xi_inner_core(1)
  npoin2D_eta_inner_core(2) = npoin2D_eta_inner_core(1)

!! DK DK added this to reduce the size of the buffers
! size of buffers is the sum of two sizes because we handle two regions in the same MPI call
  npoin2D_max_all = max(maxval(npoin2D_xi_crust_mantle(:) + npoin2D_xi_inner_core(:)), &
                        maxval(npoin2D_eta_crust_mantle(:) + npoin2D_eta_inner_core(:)))
  if(FEWER_MESSAGES_LARGER_BUFFERS) then
    NDIM_smaller_buffers = NDIM
  else
    NDIM_smaller_buffers = 1
  endif

! read the number of receivers
  rec_filename = 'DATA/STATIONS'
  call get_value_string(STATIONS, 'solver.STATIONS', rec_filename)
! get total number of receivers
  if(myrank == 0) then
    open(unit=IIN,file=STATIONS,iostat=ios,status='old',action='read')
    nrec = 0
    do while(ios == 0)
      read(IIN,"(a)",iostat=ios) dummystring
      if(ios == 0) nrec = nrec + 1
    enddo
    close(IIN)
  endif
! broadcast the information read on the master to the nodes
  call MPI_BCAST(nrec,1,MPI_INTEGER,0,MPI_COMM_WORLD,ier)

!! DK DK for the merged version
!!!!!!!! DK DK solver inserted here
!!!!!!!! DK DK solver inserted here
!!!!!!!! DK DK solver inserted here
  include 'call_specfem1.f90'

! synchronize all the processes to make sure everybody has finished
  call MPI_BARRIER(MPI_COMM_WORLD,ier)

! stop all the MPI processes, and exit
  call MPI_FINALIZE(ier)

  end program main_program

