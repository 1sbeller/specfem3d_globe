!=====================================================================
!
!          S p e c f e m 3 D  G l o b e  V e r s i o n  4 . 1
!          --------------------------------------------------
!
!          Main authors: Dimitri Komatitsch and Jeroen Tromp
!    Seismological Laboratory, California Institute of Technology, USA
!             and University of Pau / CNRS / INRIA, France
! (c) California Institute of Technology and University of Pau / CNRS / INRIA
!                            August 2008
!
! This program is free software; you can redistribute it and/or modify
! it under the terms of the GNU General Public License as published by
! the Free Software Foundation; either version 2 of the License, or
! (at your option) any later version.
!
! This program is distributed in the hope that it will be useful,
! but WITHOUT ANY WARRANTY; without even the implied warranty of
! MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
! GNU General Public License for more details.
!
! You should have received a copy of the GNU General Public License along
! with this program; if not, write to the Free Software Foundation, Inc.,
! 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
!
!=====================================================================

!=======================================================================!
!   specfem3D is a 3-D spectral-element solver for the Earth.           !
!   It uses a mesh generated by program meshfem3D                       !
!=======================================================================!

!! DK DK for the merged version
  include 'call_specfem2.f90'

  use dyn_array

  implicit none

! standard include of the MPI library
  include 'mpif.h'

!!!!!!!!!!! DK DK now in module dyn_array  include "constants.h"
  include "precision.h"

! include values created by the mesher
  include "values_from_mesher.h"

! attenuation_model_variables
  type attenuation_model_variables
    sequence
    double precision min_period, max_period
    double precision                          :: QT_c_source        ! Source Frequency
    double precision, dimension(N_SLS)        :: Qtau_s             ! tau_sigma
    double precision, dimension(:), pointer   :: QrDisc             ! Discontinutitues Defined
    double precision, dimension(:), pointer   :: Qr                 ! Radius
    integer, dimension(:), pointer            :: interval_Q         ! Steps
    double precision, dimension(:), pointer   :: Qmu                ! Shear Attenuation
    double precision, dimension(:,:), pointer :: Qtau_e             ! tau_epsilon
    double precision, dimension(:), pointer   :: Qomsb, Qomsb2      ! one_minus_sum_beta
    double precision, dimension(:,:), pointer :: Qfc, Qfc2          ! factor_common
    double precision, dimension(:), pointer   :: Qsf, Qsf2          ! scale_factor
    integer, dimension(:), pointer            :: Qrmin              ! Max and Mins of idoubling
    integer, dimension(:), pointer            :: Qrmax              ! Max and Mins of idoubling
    integer                                   :: Qn                 ! Number of points
  end type attenuation_model_variables

  type (attenuation_model_variables) AM_V
! attenuation_model_variables

! memory variables and standard linear solids for attenuation
  double precision, dimension(N_SLS) :: tau_sigma_dble
  double precision, dimension(ATT1,ATT2,ATT3,ATT4) :: omsb_crust_mantle_dble, factor_scale_crust_mantle_dble
  double precision, dimension(ATT1,ATT2,ATT3,ATT5) :: omsb_inner_core_dble, factor_scale_inner_core_dble
  real(kind=CUSTOM_REAL), dimension(ATT1,ATT2,ATT3,ATT4) :: one_minus_sum_beta_crust_mantle, factor_scale_crust_mantle
  real(kind=CUSTOM_REAL), dimension(ATT1,ATT2,ATT3,ATT5) :: one_minus_sum_beta_inner_core, factor_scale_inner_core

  real(kind=CUSTOM_REAL) mul

  double precision, dimension(N_SLS) :: alphaval_dble, betaval_dble, gammaval_dble
  real(kind=CUSTOM_REAL), dimension(N_SLS) :: alphaval, betaval, gammaval
  real(kind=CUSTOM_REAL), dimension(N_SLS,ATT1,ATT2,ATT3,ATT4) :: factor_common_crust_mantle
  real(kind=CUSTOM_REAL), dimension(N_SLS,ATT1,ATT2,ATT3,ATT5) :: factor_common_inner_core
  double precision, dimension(N_SLS,ATT1,ATT2,ATT3,ATT4) :: factor_common_crust_mantle_dble
  double precision, dimension(N_SLS,ATT1,ATT2,ATT3,ATT5) :: factor_common_inner_core_dble

  double precision scale_factor,scale_factor_minus_one
  real(kind=CUSTOM_REAL) dist_cr

  real(kind=CUSTOM_REAL), dimension(5,N_SLS,NGLLX,NGLLY,NGLLZ,NSPEC_CRUST_MANTLE_ATTENUAT) :: R_memory_crust_mantle
  real(kind=CUSTOM_REAL), dimension(5,NGLLX,NGLLY,NGLLZ,NSPEC_CRUST_MANTLE_STR_OR_ATT) :: epsilondev_crust_mantle

  real(kind=CUSTOM_REAL), dimension(5,N_SLS,NGLLX,NGLLY,NGLLZ,NSPEC_INNER_CORE_ATTENUATION) :: R_memory_inner_core
  real(kind=CUSTOM_REAL), dimension(5,NGLLX,NGLLY,NGLLZ,NSPEC_INNER_CORE_STR_OR_ATT) :: epsilondev_inner_core

! for matching with central cube in inner core
  integer, dimension(:), allocatable :: sender_from_slices_to_cube
  integer, dimension(:,:), allocatable :: ibool_central_cube
  double precision, dimension(:,:), allocatable :: buffer_slices,buffer_slices2
  double precision, dimension(:,:,:), allocatable :: buffer_all_cube_from_slices
  integer nb_msgs_theor_in_cube,non_zero_nb_msgs_theor_in_cube,npoin2D_cube_from_slices,receiver_cube_from_slices

  integer nspec2D_xmin_inner_core,nspec2D_xmax_inner_core,nspec2D_ymin_inner_core,nspec2D_ymax_inner_core,ndim_assemble

! use integer array to store values
  integer, dimension(NX_BATHY,NY_BATHY) :: ibathy_topo

! for crust/oceans coupling
  integer, dimension(NSPEC2D_BOTTOM_CM) :: ibelm_bottom_crust_mantle

! additional mass matrix for ocean load
  real(kind=CUSTOM_REAL), dimension(NGLOB_CRUST_MANTLE_OCEANS) :: rmass_ocean_load
  real(kind=CUSTOM_REAL), dimension(NDIM,NGLLX,NGLLY,NSPEC2D_TOP_CM) :: normal_top_crust_mantle
  integer, dimension(NSPEC2D_TOP_CM) :: ibelm_top_crust_mantle

! flag to mask ocean-bottom degrees of freedom for ocean load
  logical, dimension(NGLOB_CRUST_MANTLE_OCEANS) :: updated_dof_ocean_load

  real(kind=CUSTOM_REAL) additional_term,force_normal_comp

! arrays to couple with the fluid regions by pointwise matching
  integer, dimension(NSPEC2D_BOTTOM_OC) :: ibelm_bottom_outer_core
  integer, dimension(NSPEC2D_TOP_OC) :: ibelm_top_outer_core

  real(kind=CUSTOM_REAL), dimension(NDIM,NGLLX,NGLLY,NSPEC2D_BOTTOM_OC) :: normal_bottom_outer_core
  real(kind=CUSTOM_REAL), dimension(NDIM,NGLLX,NGLLY,NSPEC2D_TOP_OC) :: normal_top_outer_core

  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NSPEC2D_BOTTOM_OC) :: jacobian2D_bottom_outer_core
  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NSPEC2D_TOP_OC) :: jacobian2D_top_outer_core

  integer, dimension(NSPEC2DMAX_XMIN_XMAX_IC) :: ibelm_xmin_inner_core,ibelm_xmax_inner_core
  integer, dimension(NSPEC2DMAX_YMIN_YMAX_IC) :: ibelm_ymin_inner_core,ibelm_ymax_inner_core
  integer, dimension(NSPEC2D_BOTTOM_IC) :: ibelm_bottom_inner_core
  integer, dimension(NSPEC2D_TOP_IC) :: ibelm_top_inner_core

! for matching between fluid and solid regions
  integer :: ispec2D,k_corresp,ispec_selected
  real(kind=CUSTOM_REAL) :: displ_x,displ_y,displ_z,nx,ny,nz,displ_n,weight,pressure

! for ellipticity
  integer nspl
  double precision rspl(NR),espl(NR),espl2(NR)

! for conversion from x y z to r theta phi
  real(kind=CUSTOM_REAL) rval,thetaval,phival

! ---- arrays to assemble between chunks

! communication pattern for faces between chunks
  integer, dimension(NUMMSGS_FACES_VAL) :: iprocfrom_faces,iprocto_faces,imsg_type

! communication pattern for corners between chunks
  integer, dimension(NCORNERSCHUNKS_VAL) :: iproc_master_corners,iproc_worker1_corners,iproc_worker2_corners

! indirect addressing for each message for faces and corners of the chunks
! a given slice can belong to at most one corner and at most two faces
  integer, dimension(NGLOB2DMAX_XY_VAL_CM,NUMFACES_SHARED) :: iboolfaces_crust_mantle
  integer, dimension(NGLOB2DMAX_XY_VAL_OC,NUMFACES_SHARED) :: iboolfaces_outer_core
  integer, dimension(NGLOB2DMAX_XY_VAL_IC,NUMFACES_SHARED) :: iboolfaces_inner_core

! buffers for send and receive between faces of the slices and the chunks
! we use the same buffers to assemble scalars and vectors because vectors are
! always three times bigger and therefore scalars can use the first part
! of the vector buffer in memory even if it has an additional index here
! allocate these automatic arrays in the memory stack to avoid memory fragmentation with "allocate()"
  real(kind=CUSTOM_REAL), dimension(NDIM_smaller_buffers,npoin2D_max_all) :: buffer_send_faces,buffer_received_faces

! -------- arrays specific to each region here -----------

! ----------------- crust, mantle and oceans ---------------------

! mesh parameters
  integer, dimension(NGLLX,NGLLY,NGLLZ,NSPEC_CRUST_MANTLE) :: ibool_crust_mantle

  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NGLLZ,NSPEC_CRUST_MANTLE) :: &
        xix_crust_mantle,xiy_crust_mantle,xiz_crust_mantle,&
        etax_crust_mantle,etay_crust_mantle,etaz_crust_mantle, &
        gammax_crust_mantle,gammay_crust_mantle,gammaz_crust_mantle
  real(kind=CUSTOM_REAL), dimension(NGLOB_CRUST_MANTLE) :: &
        xstore_crust_mantle,ystore_crust_mantle,zstore_crust_mantle

! arrays for isotropic elements stored only where needed to save space
  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NGLLZ,NSPECMAX_ISO_MANTLE) :: &
        kappavstore_crust_mantle,muvstore_crust_mantle

! arrays for anisotropic elements stored only where needed to save space
  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NGLLZ,NSPECMAX_TISO_MANTLE) :: &
        kappahstore_crust_mantle,muhstore_crust_mantle,eta_anisostore_crust_mantle

! arrays for full anisotropy only when needed
  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NGLLZ,NSPECMAX_ANISO_MANTLE) :: &
        c11store_crust_mantle,c12store_crust_mantle,c13store_crust_mantle, &
        c22store_crust_mantle,c23store_crust_mantle,c33store_crust_mantle, &
        c44store_crust_mantle,c55store_crust_mantle,c66store_crust_mantle

! local to global mapping
  integer, dimension(NSPEC_CRUST_MANTLE) :: idoubling_crust_mantle

! mass matrix
  real(kind=CUSTOM_REAL), dimension(NGLOB_CRUST_MANTLE) :: rmass_crust_mantle

! displacement, velocity, acceleration
  real(kind=CUSTOM_REAL), dimension(NDIM,NGLOB_CRUST_MANTLE) :: &
     displ_crust_mantle,veloc_crust_mantle,accel_crust_mantle

! ----------------- outer core ---------------------

! mesh parameters
  integer, dimension(NGLLX,NGLLY,NGLLZ,NSPEC_OUTER_CORE) :: ibool_outer_core

  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NGLLZ,NSPEC_OUTER_CORE) :: &
        xix_outer_core,xiy_outer_core,xiz_outer_core,&
        etax_outer_core,etay_outer_core,etaz_outer_core, &
        gammax_outer_core,gammay_outer_core,gammaz_outer_core
  real(kind=CUSTOM_REAL), dimension(NGLOB_OUTER_CORE) :: &
        xstore_outer_core,ystore_outer_core,zstore_outer_core

! mass matrix
  real(kind=CUSTOM_REAL), dimension(NGLOB_OUTER_CORE) :: rmass_outer_core

! velocity potential
  real(kind=CUSTOM_REAL), dimension(NGLOB_OUTER_CORE) :: displ_outer_core, &
    veloc_outer_core,accel_outer_core

! ----------------- inner core ---------------------

! mesh parameters
  integer, dimension(NGLLX,NGLLY,NGLLZ,NSPEC_INNER_CORE) :: ibool_inner_core

  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NGLLZ,NSPEC_INNER_CORE) :: &
        xix_inner_core,xiy_inner_core,xiz_inner_core,&
        etax_inner_core,etay_inner_core,etaz_inner_core, &
        gammax_inner_core,gammay_inner_core,gammaz_inner_core, &
        kappavstore_inner_core,muvstore_inner_core
  real(kind=CUSTOM_REAL), dimension(NGLOB_INNER_CORE) :: &
        xstore_inner_core,ystore_inner_core,zstore_inner_core

! arrays for inner-core anisotropy only when needed
  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY,NGLLZ,NSPECMAX_ANISO_IC) :: &
        c11store_inner_core,c33store_inner_core,c12store_inner_core, &
        c13store_inner_core,c44store_inner_core

! local to global mapping
  integer, dimension(NSPEC_INNER_CORE) :: idoubling_inner_core

! mass matrix
  real(kind=CUSTOM_REAL), dimension(NGLOB_INNER_CORE) :: rmass_inner_core

! displacement, velocity, acceleration
  real(kind=CUSTOM_REAL), dimension(NDIM,NGLOB_INNER_CORE) :: &
     displ_inner_core,veloc_inner_core,accel_inner_core

! Newmark time scheme parameters and non-dimensionalization
  real(kind=CUSTOM_REAL) time,deltat,deltatover2,deltatsqover2
  double precision scale_t,scale_displ,scale_veloc

  integer npoin2D_faces_crust_mantle(NUMFACES_SHARED)
  integer npoin2D_faces_outer_core(NUMFACES_SHARED)
  integer npoin2D_faces_inner_core(NUMFACES_SHARED)

! parameters for the source
  integer it,isource
  integer yr,jda,ho,mi
  real(kind=CUSTOM_REAL) stf_used
  double precision sec,stf
  double precision t0
  double precision, external :: comp_source_time_function

! allocate these automatic arrays in the memory stack to avoid memory fragmentation with "allocate()"
  integer, dimension(NSOURCES) :: islice_selected_source,ispec_selected_source
  real(kind=CUSTOM_REAL), dimension(NDIM,NGLLX,NGLLY,NGLLZ) :: sourcearray
  real(kind=CUSTOM_REAL), dimension(NSOURCES,NDIM,NGLLX,NGLLY,NGLLZ) :: sourcearrays
  double precision, dimension(NSOURCES) :: Mxx,Myy,Mzz,Mxy,Mxz,Myz
  double precision, dimension(NSOURCES) :: xi_source,eta_source,gamma_source
  double precision, dimension(NSOURCES) :: t_cmt,hdur,hdur_gaussian
  double precision, dimension(NSOURCES) :: theta_source,phi_source
  double precision, dimension(NDIM,NDIM,NSOURCES) :: nu_source

! receiver information
  integer :: nrec,nrec_local,nrec_tot_found,irec_local
  double precision :: hlagrange
  integer, dimension(:), allocatable :: number_receiver_global
  character(len=150) :: STATIONS,rec_filename

! allocate these automatic arrays in the memory stack to avoid memory fragmentation with "allocate()"
  integer, dimension(nrec) :: islice_selected_rec,ispec_selected_rec
  double precision, dimension(nrec) :: xi_receiver,eta_receiver,gamma_receiver
  double precision, dimension(NDIM,NDIM,nrec) :: nu
  double precision, dimension(nrec) :: stlat,stlon,stele
  character(len=MAX_LENGTH_STATION_NAME), dimension(nrec) :: station_name
  character(len=MAX_LENGTH_NETWORK_NAME), dimension(nrec) :: network_name

! seismograms
  integer it_begin,it_end,nit_written
  double precision uxd, uyd, uzd
  real(kind=CUSTOM_REAL), dimension(:,:,:), allocatable :: seismograms
  integer :: seismo_offset, seismo_current

  integer i,j,k,ispec,irec,iglob,iglob_mantle,iglob_inner_core

! number of faces between chunks
  integer NUM_FACES,NUMMSGS_FACES

! number of corners between chunks
  integer NCORNERSCHUNKS

! number of message types
  integer NUM_MSG_TYPES

! indirect addressing for each corner of the chunks
  integer, dimension(NGLOB1D_RADIAL_CM,NUMCORNERS_SHARED) :: iboolcorner_crust_mantle
  integer, dimension(NGLOB1D_RADIAL_OC,NUMCORNERS_SHARED) :: iboolcorner_outer_core
  integer, dimension(NGLOB1D_RADIAL_IC,NUMCORNERS_SHARED) :: iboolcorner_inner_core

! buffers for send and receive between corners of the chunks
  real(kind=CUSTOM_REAL), dimension(NGLOB1D_RADIAL_CM) :: buffer_send_chunkcorners_scalar,buffer_recv_chunkcorners_scalar
! size of buffers is the sum of two sizes because we handle two regions in the same MPI call
  real(kind=CUSTOM_REAL), dimension(NDIM,NGLOB1D_RADIAL_CM + NGLOB1D_RADIAL_IC) :: &
     buffer_send_chunkcorners_vector,buffer_recv_chunkcorners_vector

! Gauss-Lobatto-Legendre points of integration and weights
  double precision, dimension(NGLLX) :: xigll,wxgll
  double precision, dimension(NGLLY) :: yigll,wygll
  double precision, dimension(NGLLZ) :: zigll,wzgll

! product of weights for gravity term
  double precision, dimension(NGLLX,NGLLY,NGLLZ) :: wgll_cube

! array with derivatives of Lagrange polynomials and precalculated products
  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLX) :: hprime_xx,hprimewgll_xx
  real(kind=CUSTOM_REAL), dimension(NGLLY,NGLLY) :: hprime_yy,hprimewgll_yy
  real(kind=CUSTOM_REAL), dimension(NGLLZ,NGLLZ) :: hprime_zz,hprimewgll_zz
  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLY) :: wgllwgll_xy
  real(kind=CUSTOM_REAL), dimension(NGLLX,NGLLZ) :: wgllwgll_xz
  real(kind=CUSTOM_REAL), dimension(NGLLY,NGLLZ) :: wgllwgll_yz

! Lagrange interpolators at receivers
  double precision, dimension(NGLLX) :: hxir,hpxir
  double precision, dimension(NGLLY) :: hpetar,hetar
  double precision, dimension(NGLLZ) :: hgammar,hpgammar
  double precision, dimension(:,:), allocatable :: hxir_store,hetar_store,hgammar_store

! 2-D addressing and buffers for summation between slices
  integer, dimension(NGLOB2DMAX_XMIN_XMAX_CM) :: iboolleft_xi_crust_mantle,iboolright_xi_crust_mantle
  integer, dimension(NGLOB2DMAX_YMIN_YMAX_CM) :: iboolleft_eta_crust_mantle,iboolright_eta_crust_mantle

  integer, dimension(NGLOB2DMAX_XMIN_XMAX_OC) :: iboolleft_xi_outer_core,iboolright_xi_outer_core
  integer, dimension(NGLOB2DMAX_YMIN_YMAX_OC) :: iboolleft_eta_outer_core,iboolright_eta_outer_core

  integer, dimension(NGLOB2DMAX_XMIN_XMAX_IC) :: iboolleft_xi_inner_core,iboolright_xi_inner_core
  integer, dimension(NGLOB2DMAX_YMIN_YMAX_IC) :: iboolleft_eta_inner_core,iboolright_eta_inner_core

! for addressing of the slices
  integer, dimension(NCHUNKS_VAL,0:NPROC_XI_VAL-1,0:NPROC_ETA_VAL-1) :: addressing
  integer, dimension(0:NPROCTOT_VAL-1) :: ichunk_slice,iproc_xi_slice,iproc_eta_slice

! proc numbers for MPI
  integer myrank,sizeprocs,ier,errorcode

  integer, dimension(NB_SQUARE_EDGES_ONEDIR) :: npoin2D_xi_crust_mantle,npoin2D_eta_crust_mantle
  integer, dimension(NB_SQUARE_EDGES_ONEDIR) :: npoin2D_xi_outer_core,npoin2D_eta_outer_core
  integer, dimension(NB_SQUARE_EDGES_ONEDIR) :: npoin2D_xi_inner_core,npoin2D_eta_inner_core

!! DK DK added this to reduce the size of the buffers
  integer :: npoin2D_max_all,NDIM_smaller_buffers

  integer ichunk,iproc_xi,iproc_eta
  integer NPROC_ONE_DIRECTION

! maximum of the norm of the displacement and of the potential in the fluid
  real(kind=CUSTOM_REAL) Usolidnorm,Usolidnorm_all,Ufluidnorm,Ufluidnorm_all

! timer MPI
  integer :: ihours,iminutes,iseconds,int_tCPU, &
             ihours_remain,iminutes_remain,iseconds_remain,int_t_remain, &
             ihours_total,iminutes_total,iseconds_total,int_t_total

  double precision :: time_start,tCPU,t_remain,t_total

! to determine date and time at which the run will finish
  character(len=8) datein
  character(len=10) timein
  character(len=5)  :: zone
  integer, dimension(8) :: time_values
  character(len=3), dimension(12) :: month_name
  character(len=3), dimension(0:6) :: weekday_name
  data month_name /'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'/
  data weekday_name /'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat'/
  integer :: year,mon,day,hr,minutes,timestamp,julian_day_number,day_of_week, &
             timestamp_remote,year_remote,mon_remote,day_remote,hr_remote,minutes_remote,day_of_week_remote
  integer, external :: idaywk

! parameters read from parameter file
  integer MIN_ATTENUATION_PERIOD,MAX_ATTENUATION_PERIOD,NER_CRUST, &
          NER_80_MOHO,NER_220_80,NER_400_220,NER_600_400,NER_670_600,NER_771_670, &
          NER_TOPDDOUBLEPRIME_771,NER_CMB_TOPDDOUBLEPRIME,NER_OUTER_CORE, &
          NER_TOP_CENTRAL_CUBE_ICB,NEX_XI,NEX_ETA,RMOHO_FICTITIOUS_IN_MESHER, &
          NPROC_XI,NPROC_ETA,NTSTEP_BETWEEN_OUTPUT_SEISMOS,&
          NTSTEP_BETWEEN_READ_ADJSRC,NSTEP,NSOURCES,NTSTEP_BETWEEN_FRAMES, &
          NTSTEP_BETWEEN_OUTPUT_INFO,NUMBER_OF_RUNS,NUMBER_OF_THIS_RUN,NCHUNKS,SIMULATION_TYPE, &
          REFERENCE_1D_MODEL,THREE_D_MODEL,MOVIE_VOLUME_TYPE,MOVIE_START,MOVIE_STOP, &
          ifirst_layer_aniso,ilast_layer_aniso

  double precision DT,ANGULAR_WIDTH_XI_IN_DEGREES,ANGULAR_WIDTH_ETA_IN_DEGREES,CENTER_LONGITUDE_IN_DEGREES, &
          CENTER_LATITUDE_IN_DEGREES,GAMMA_ROTATION_AZIMUTH,ROCEAN,RMIDDLE_CRUST, &
          RMOHO,R80,R120,R220,R400,R600,R670,R771,RTOPDDOUBLEPRIME,RCMB,RICB, &
          R_CENTRAL_CUBE,RHO_TOP_OC,RHO_BOTTOM_OC,RHO_OCEANS,HDUR_MOVIE, &
          MOVIE_TOP,MOVIE_BOTTOM,MOVIE_WEST,MOVIE_EAST,MOVIE_NORTH,MOVIE_SOUTH

  logical TRANSVERSE_ISOTROPY,ANISOTROPIC_3D_MANTLE,ANISOTROPIC_INNER_CORE, &
          CRUSTAL,ELLIPTICITY,GRAVITY,ONE_CRUST,ROTATION,ISOTROPIC_3D_MANTLE, &
          TOPOGRAPHY,OCEANS,MOVIE_SURFACE,MOVIE_VOLUME,MOVIE_COARSE,ATTENUATION_3D, &
          RECEIVERS_CAN_BE_BURIED,PRINT_SOURCE_TIME_FUNCTION, &
          SAVE_MESH_FILES,ATTENUATION, &
          ABSORBING_CONDITIONS,INCLUDE_CENTRAL_CUBE,INFLATE_CENTRAL_CUBE,SAVE_FORWARD, &
          OUTPUT_SEISMOS_ASCII_TEXT,OUTPUT_SEISMOS_SAC_ALPHANUM,OUTPUT_SEISMOS_SAC_BINARY, &
          ROTATE_SEISMOGRAMS_RT,HONOR_1D_SPHERICAL_MOHO,WRITE_SEISMOGRAMS_BY_MASTER,&
          SAVE_ALL_SEISMOS_IN_ONE_FILE,USE_BINARY_FOR_LARGE_FILE

  character(len=150) OUTPUT_FILES,MODEL

! parameters deduced from parameters read from file
  integer NPROC,NPROCTOT,NEX_PER_PROC_XI,NEX_PER_PROC_ETA,ratio_divide_central_cube

  integer, external :: err_occurred

  logical COMPUTE_AND_STORE_STRAIN

! for SAC headers for seismograms
  integer NSOURCES_SAC,yr_SAC,jda_SAC,ho_SAC,mi_SAC
  real mb_SAC
  double precision t_cmt_SAC,elat_SAC,elon_SAC,depth_SAC,cmt_lat_SAC,cmt_lon_SAC,cmt_depth_SAC,cmt_hdur_SAC,sec_SAC
  character(len=12) ename_SAC

! this for all the regions
  integer, dimension(MAX_NUM_REGIONS) :: NSPEC_computed, &
               NSPEC2D_XI, NSPEC2D_ETA, &
               NSPEC2DMAX_XMIN_XMAX,NSPEC2DMAX_YMIN_YMAX, &
               NSPEC2D_BOTTOM,NSPEC2D_TOP, &
               NSPEC1D_RADIAL,NGLOB1D_RADIAL, &
               NGLOB2DMAX_XMIN_XMAX,NGLOB2DMAX_YMIN_YMAX, &
               NGLOB_computed

! lookup table every km for gravity
  integer int_radius,idoubling
  double precision radius,rho,drhodr,vp,vs,Qkappa,Qmu
  double precision, dimension(NRAD_GRAVITY) :: d_ln_density_dr_table

! names of the data files for all the processors in MPI
  character(len=150) outputname

  integer iregion_selected

! computed in read_compute_parameters
  integer, dimension(MAX_NUMBER_OF_MESH_LAYERS) :: ner,ratio_sampling_array
  integer, dimension(MAX_NUMBER_OF_MESH_LAYERS) :: doubling_index
  double precision, dimension(MAX_NUMBER_OF_MESH_LAYERS) :: r_bottom,r_top
  logical, dimension(MAX_NUMBER_OF_MESH_LAYERS) :: this_layer_has_a_doubling
  double precision, dimension(MAX_NUMBER_OF_MESH_LAYERS) :: rmins,rmaxs
  logical :: CASE_3D

! arrays for BCAST
  integer, dimension(40) :: bcast_integer
  double precision, dimension(30) :: bcast_double_precision
  logical, dimension(33) :: bcast_logical

  logical :: CUT_SUPERBRICK_XI,CUT_SUPERBRICK_ETA
  integer, dimension(NB_SQUARE_CORNERS,NB_CUT_CASE) :: DIFF_NSPEC1D_RADIAL
  integer, dimension(NB_SQUARE_EDGES_ONEDIR,NB_CUT_CASE) :: DIFF_NSPEC2D_XI,DIFF_NSPEC2D_ETA

! allocate this automatic array in the memory stack to avoid memory fragmentation with "allocate()"
 real(kind=CUSTOM_REAL), dimension(NDIM,NTSTEP_BETWEEN_OUTPUT_SEISMOS) :: one_seismogram

! ************** PROGRAM STARTS HERE **************

! set up GLL points, weights and derivation matrices
  call define_derivation_matrices(xigll,yigll,zigll,wxgll,wygll,wzgll, &
         hprime_xx,hprime_yy,hprime_zz, &
         hprimewgll_xx,hprimewgll_yy,hprimewgll_zz, &
         wgllwgll_xy,wgllwgll_xz,wgllwgll_yz,wgll_cube)

!! DK DK recompute arrays here for merged version
  call recompute_missing_arrays(myrank, &
     xix_crust_mantle,xiy_crust_mantle,xiz_crust_mantle, &
     etax_crust_mantle,etay_crust_mantle,etaz_crust_mantle, &
     gammax_crust_mantle,gammay_crust_mantle,gammaz_crust_mantle, &
     xstore_crust_mantle,ystore_crust_mantle,zstore_crust_mantle, &
     xelm_store_crust_mantle,yelm_store_crust_mantle,zelm_store_crust_mantle, &
     ibool_crust_mantle,NSPEC_CRUST_MANTLE,NGLOB_CRUST_MANTLE, &
     xigll,yigll,zigll)

  call recompute_missing_arrays(myrank, &
     xix_outer_core,xiy_outer_core,xiz_outer_core, &
     etax_outer_core,etay_outer_core,etaz_outer_core, &
     gammax_outer_core,gammay_outer_core,gammaz_outer_core, &
     xstore_outer_core,ystore_outer_core,zstore_outer_core, &
     xelm_store_outer_core,yelm_store_outer_core,zelm_store_outer_core, &
     ibool_outer_core,NSPEC_OUTER_CORE,NGLOB_OUTER_CORE, &
     xigll,yigll,zigll)

  call recompute_missing_arrays(myrank, &
     xix_inner_core,xiy_inner_core,xiz_inner_core, &
     etax_inner_core,etay_inner_core,etaz_inner_core, &
     gammax_inner_core,gammay_inner_core,gammaz_inner_core, &
     xstore_inner_core,ystore_inner_core,zstore_inner_core, &
     xelm_store_inner_core,yelm_store_inner_core,zelm_store_inner_core, &
     ibool_inner_core,NSPEC_INNER_CORE,NGLOB_INNER_CORE, &
     xigll,yigll,zigll)

!! DK DK for merged version, deallocate arrays that have become useless
  deallocate(xelm_store_crust_mantle)
  deallocate(yelm_store_crust_mantle)
  deallocate(zelm_store_crust_mantle)

  deallocate(xelm_store_outer_core)
  deallocate(yelm_store_outer_core)
  deallocate(zelm_store_outer_core)

  deallocate(xelm_store_inner_core)
  deallocate(yelm_store_inner_core)
  deallocate(zelm_store_inner_core)

  if (myrank == 0) then

! read the parameter file and compute additional parameters
  call read_compute_parameters(MIN_ATTENUATION_PERIOD,MAX_ATTENUATION_PERIOD,NER_CRUST, &
         NER_80_MOHO,NER_220_80,NER_400_220,NER_600_400,NER_670_600,NER_771_670, &
         NER_TOPDDOUBLEPRIME_771,NER_CMB_TOPDDOUBLEPRIME,NER_OUTER_CORE, &
         NER_TOP_CENTRAL_CUBE_ICB,NEX_XI,NEX_ETA,RMOHO_FICTITIOUS_IN_MESHER, &
         NPROC_XI,NPROC_ETA,NTSTEP_BETWEEN_OUTPUT_SEISMOS, &
         NTSTEP_BETWEEN_READ_ADJSRC,NSTEP,NTSTEP_BETWEEN_FRAMES, &
         NTSTEP_BETWEEN_OUTPUT_INFO,NUMBER_OF_RUNS,NUMBER_OF_THIS_RUN,NCHUNKS,DT, &
         ANGULAR_WIDTH_XI_IN_DEGREES,ANGULAR_WIDTH_ETA_IN_DEGREES,CENTER_LONGITUDE_IN_DEGREES, &
         CENTER_LATITUDE_IN_DEGREES,GAMMA_ROTATION_AZIMUTH,ROCEAN,RMIDDLE_CRUST, &
         RMOHO,R80,R120,R220,R400,R600,R670,R771,RTOPDDOUBLEPRIME,RCMB,RICB, &
         R_CENTRAL_CUBE,RHO_TOP_OC,RHO_BOTTOM_OC,RHO_OCEANS,HDUR_MOVIE,MOVIE_VOLUME_TYPE, &
         MOVIE_TOP,MOVIE_BOTTOM,MOVIE_WEST,MOVIE_EAST,MOVIE_NORTH,MOVIE_SOUTH,MOVIE_START,MOVIE_STOP, &
         TRANSVERSE_ISOTROPY,ANISOTROPIC_3D_MANTLE, &
         ANISOTROPIC_INNER_CORE,CRUSTAL,ELLIPTICITY,GRAVITY,ONE_CRUST, &
         ROTATION,ISOTROPIC_3D_MANTLE,TOPOGRAPHY,OCEANS,MOVIE_SURFACE, &
         MOVIE_VOLUME,MOVIE_COARSE,ATTENUATION_3D,RECEIVERS_CAN_BE_BURIED, &
         PRINT_SOURCE_TIME_FUNCTION,SAVE_MESH_FILES, &
         ATTENUATION,REFERENCE_1D_MODEL,THREE_D_MODEL,ABSORBING_CONDITIONS, &
         INCLUDE_CENTRAL_CUBE,INFLATE_CENTRAL_CUBE,MODEL,SIMULATION_TYPE,SAVE_FORWARD, &
         NPROC,NPROCTOT,NEX_PER_PROC_XI,NEX_PER_PROC_ETA, &
         NSPEC_computed, &
         NSPEC2D_XI, &
         NSPEC2D_ETA, &
         NSPEC2DMAX_XMIN_XMAX,NSPEC2DMAX_YMIN_YMAX,NSPEC2D_BOTTOM,NSPEC2D_TOP, &
         NSPEC1D_RADIAL,NGLOB1D_RADIAL, &
         NGLOB2DMAX_XMIN_XMAX,NGLOB2DMAX_YMIN_YMAX, &
         NGLOB_computed, &
         ratio_sampling_array, ner, doubling_index,r_bottom,r_top,this_layer_has_a_doubling,rmins,rmaxs,CASE_3D, &
         OUTPUT_SEISMOS_ASCII_TEXT,OUTPUT_SEISMOS_SAC_ALPHANUM,OUTPUT_SEISMOS_SAC_BINARY, &
         ROTATE_SEISMOGRAMS_RT,ratio_divide_central_cube,HONOR_1D_SPHERICAL_MOHO,CUT_SUPERBRICK_XI,CUT_SUPERBRICK_ETA,&
         DIFF_NSPEC1D_RADIAL,DIFF_NSPEC2D_XI,DIFF_NSPEC2D_ETA,&
         WRITE_SEISMOGRAMS_BY_MASTER,SAVE_ALL_SEISMOS_IN_ONE_FILE, &
         USE_BINARY_FOR_LARGE_FILE,ifirst_layer_aniso,ilast_layer_aniso,.false.)

    if(err_occurred() /= 0) call exit_MPI(myrank,'an error occurred while reading the parameter file')

    bcast_integer = (/MIN_ATTENUATION_PERIOD,MAX_ATTENUATION_PERIOD,NER_CRUST, &
            NER_80_MOHO,NER_220_80,NER_400_220,NER_600_400,NER_670_600,NER_771_670, &
            NER_TOPDDOUBLEPRIME_771,NER_CMB_TOPDDOUBLEPRIME,NER_OUTER_CORE, &
            NER_TOP_CENTRAL_CUBE_ICB,NEX_XI,NEX_ETA,RMOHO_FICTITIOUS_IN_MESHER, &
            NPROC_XI,NPROC_ETA,NTSTEP_BETWEEN_OUTPUT_SEISMOS, &
            NTSTEP_BETWEEN_READ_ADJSRC,NSTEP,NSOURCES,NTSTEP_BETWEEN_FRAMES, &
            NTSTEP_BETWEEN_OUTPUT_INFO,NUMBER_OF_RUNS,NUMBER_OF_THIS_RUN,NCHUNKS,&
            SIMULATION_TYPE,REFERENCE_1D_MODEL,THREE_D_MODEL,NPROC,NPROCTOT, &
            NEX_PER_PROC_XI,NEX_PER_PROC_ETA,ratio_divide_central_cube,&
            MOVIE_VOLUME_TYPE,MOVIE_START,MOVIE_STOP,ifirst_layer_aniso,ilast_layer_aniso/)

    bcast_logical = (/TRANSVERSE_ISOTROPY,ANISOTROPIC_3D_MANTLE,ANISOTROPIC_INNER_CORE, &
            CRUSTAL,ELLIPTICITY,GRAVITY,ONE_CRUST,ROTATION,ISOTROPIC_3D_MANTLE, &
            TOPOGRAPHY,OCEANS,MOVIE_SURFACE,MOVIE_VOLUME,MOVIE_COARSE,ATTENUATION_3D, &
            RECEIVERS_CAN_BE_BURIED,PRINT_SOURCE_TIME_FUNCTION, &
            SAVE_MESH_FILES,ATTENUATION, &
            ABSORBING_CONDITIONS,INCLUDE_CENTRAL_CUBE,INFLATE_CENTRAL_CUBE,SAVE_FORWARD,CASE_3D, &
            OUTPUT_SEISMOS_ASCII_TEXT,OUTPUT_SEISMOS_SAC_ALPHANUM,OUTPUT_SEISMOS_SAC_BINARY, &
            ROTATE_SEISMOGRAMS_RT,CUT_SUPERBRICK_XI,CUT_SUPERBRICK_ETA,&
            WRITE_SEISMOGRAMS_BY_MASTER,SAVE_ALL_SEISMOS_IN_ONE_FILE,USE_BINARY_FOR_LARGE_FILE/)

    bcast_double_precision = (/DT,ANGULAR_WIDTH_XI_IN_DEGREES,ANGULAR_WIDTH_ETA_IN_DEGREES,CENTER_LONGITUDE_IN_DEGREES, &
            CENTER_LATITUDE_IN_DEGREES,GAMMA_ROTATION_AZIMUTH,ROCEAN,RMIDDLE_CRUST, &
            RMOHO,R80,R120,R220,R400,R600,R670,R771,RTOPDDOUBLEPRIME,RCMB,RICB, &
            R_CENTRAL_CUBE,RHO_TOP_OC,RHO_BOTTOM_OC,RHO_OCEANS,HDUR_MOVIE, &
            MOVIE_TOP,MOVIE_BOTTOM,MOVIE_WEST,MOVIE_EAST,MOVIE_NORTH,MOVIE_SOUTH/)

  endif

! broadcast the information read on the master to the nodes
    call MPI_BCAST(bcast_integer,40,MPI_INTEGER,0,MPI_COMM_WORLD,ier)

    call MPI_BCAST(bcast_double_precision,30,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,ier)

    call MPI_BCAST(bcast_logical,33,MPI_LOGICAL,0,MPI_COMM_WORLD,ier)

    call MPI_BCAST(MODEL,150,MPI_CHARACTER,0,MPI_COMM_WORLD,ier)

    call MPI_BCAST(ner,MAX_NUMBER_OF_MESH_LAYERS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(ratio_sampling_array,MAX_NUMBER_OF_MESH_LAYERS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(doubling_index,MAX_NUMBER_OF_MESH_LAYERS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)

    call MPI_BCAST(r_bottom,MAX_NUMBER_OF_MESH_LAYERS,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(r_top,MAX_NUMBER_OF_MESH_LAYERS,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(rmins,MAX_NUMBER_OF_MESH_LAYERS,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(rmaxs,MAX_NUMBER_OF_MESH_LAYERS,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,ier)

    call MPI_BCAST(this_layer_has_a_doubling,MAX_NUMBER_OF_MESH_LAYERS,MPI_LOGICAL,0,MPI_COMM_WORLD,ier)

    call MPI_BCAST(NSPEC_computed,MAX_NUM_REGIONS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(NSPEC2D_XI,MAX_NUM_REGIONS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(NSPEC2D_ETA,MAX_NUM_REGIONS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(NSPEC2DMAX_XMIN_XMAX,MAX_NUM_REGIONS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(NSPEC2DMAX_YMIN_YMAX,MAX_NUM_REGIONS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(NSPEC2D_BOTTOM,MAX_NUM_REGIONS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(NSPEC2D_TOP,MAX_NUM_REGIONS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(NSPEC1D_RADIAL,MAX_NUM_REGIONS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(NGLOB1D_RADIAL,MAX_NUM_REGIONS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(NGLOB2DMAX_XMIN_XMAX,MAX_NUM_REGIONS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(NGLOB2DMAX_YMIN_YMAX,MAX_NUM_REGIONS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(NGLOB_computed,MAX_NUM_REGIONS,MPI_INTEGER,0,MPI_COMM_WORLD,ier)

    call MPI_BCAST(DIFF_NSPEC1D_RADIAL,NB_SQUARE_CORNERS*NB_CUT_CASE,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(DIFF_NSPEC2D_ETA,NB_SQUARE_EDGES_ONEDIR*NB_CUT_CASE,MPI_INTEGER,0,MPI_COMM_WORLD,ier)
    call MPI_BCAST(DIFF_NSPEC2D_XI,NB_SQUARE_EDGES_ONEDIR*NB_CUT_CASE,MPI_INTEGER,0,MPI_COMM_WORLD,ier)

  if (myrank /=0) then

    MIN_ATTENUATION_PERIOD = bcast_integer(1)
    MAX_ATTENUATION_PERIOD = bcast_integer(2)
    NER_CRUST = bcast_integer(3)
    NER_80_MOHO = bcast_integer(4)
    NER_220_80 = bcast_integer(5)
    NER_400_220 = bcast_integer(6)
    NER_600_400 = bcast_integer(7)
    NER_670_600 = bcast_integer(8)
    NER_771_670 = bcast_integer(9)
    NER_TOPDDOUBLEPRIME_771 = bcast_integer(10)
    NER_CMB_TOPDDOUBLEPRIME = bcast_integer(11)
    NER_OUTER_CORE = bcast_integer(12)
    NER_TOP_CENTRAL_CUBE_ICB = bcast_integer(13)
    NEX_XI = bcast_integer(14)
    NEX_ETA = bcast_integer(15)
    RMOHO_FICTITIOUS_IN_MESHER = bcast_integer(16)
    NPROC_XI = bcast_integer(17)
    NPROC_ETA = bcast_integer(18)
    NTSTEP_BETWEEN_OUTPUT_SEISMOS = bcast_integer(19)
    NTSTEP_BETWEEN_READ_ADJSRC = bcast_integer(20)
    NSTEP = bcast_integer(21)
    NSOURCES = bcast_integer(22)
    NTSTEP_BETWEEN_FRAMES = bcast_integer(23)
    NTSTEP_BETWEEN_OUTPUT_INFO = bcast_integer(24)
    NUMBER_OF_RUNS = bcast_integer(25)
    NUMBER_OF_THIS_RUN = bcast_integer(26)
    NCHUNKS = bcast_integer(27)
    SIMULATION_TYPE = bcast_integer(28)
    REFERENCE_1D_MODEL = bcast_integer(29)
    THREE_D_MODEL = bcast_integer(30)
    NPROC = bcast_integer(31)
    NPROCTOT = bcast_integer(32)
    NEX_PER_PROC_XI = bcast_integer(33)
    NEX_PER_PROC_ETA = bcast_integer(34)
    ratio_divide_central_cube = bcast_integer(35)
    MOVIE_VOLUME_TYPE = bcast_integer(36)
    MOVIE_START = bcast_integer(37)
    MOVIE_STOP = bcast_integer(38)
    ifirst_layer_aniso = bcast_integer(39)
    ilast_layer_aniso = bcast_integer(40)

    TRANSVERSE_ISOTROPY = bcast_logical(1)
    ANISOTROPIC_3D_MANTLE = bcast_logical(2)
    ANISOTROPIC_INNER_CORE = bcast_logical(3)
    CRUSTAL = bcast_logical(4)
    ELLIPTICITY = bcast_logical(5)
    GRAVITY = bcast_logical(6)
    ONE_CRUST = bcast_logical(7)
    ROTATION = bcast_logical(8)
    ISOTROPIC_3D_MANTLE = bcast_logical(9)
    TOPOGRAPHY = bcast_logical(10)
    OCEANS = bcast_logical(11)
    MOVIE_SURFACE = bcast_logical(12)
    MOVIE_VOLUME = bcast_logical(13)
    MOVIE_COARSE = bcast_logical(14)
    ATTENUATION_3D = bcast_logical(15)
    RECEIVERS_CAN_BE_BURIED = bcast_logical(16)
    PRINT_SOURCE_TIME_FUNCTION = bcast_logical(17)
    SAVE_MESH_FILES = bcast_logical(18)
    ATTENUATION = bcast_logical(19)
    ABSORBING_CONDITIONS = bcast_logical(20)
    INCLUDE_CENTRAL_CUBE = bcast_logical(21)
    INFLATE_CENTRAL_CUBE = bcast_logical(22)
    SAVE_FORWARD = bcast_logical(23)
    CASE_3D = bcast_logical(24)
    OUTPUT_SEISMOS_ASCII_TEXT = bcast_logical(25)
    OUTPUT_SEISMOS_SAC_ALPHANUM = bcast_logical(26)
    OUTPUT_SEISMOS_SAC_BINARY = bcast_logical(27)
    ROTATE_SEISMOGRAMS_RT = bcast_logical(28)
    CUT_SUPERBRICK_XI = bcast_logical(29)
    CUT_SUPERBRICK_ETA = bcast_logical(30)
    WRITE_SEISMOGRAMS_BY_MASTER = bcast_logical(31)
    SAVE_ALL_SEISMOS_IN_ONE_FILE = bcast_logical(32)
    USE_BINARY_FOR_LARGE_FILE = bcast_logical(33)

    DT = bcast_double_precision(1)
    ANGULAR_WIDTH_XI_IN_DEGREES = bcast_double_precision(2)
    ANGULAR_WIDTH_ETA_IN_DEGREES = bcast_double_precision(3)
    CENTER_LONGITUDE_IN_DEGREES = bcast_double_precision(4)
    CENTER_LATITUDE_IN_DEGREES = bcast_double_precision(5)
    GAMMA_ROTATION_AZIMUTH = bcast_double_precision(6)
    ROCEAN = bcast_double_precision(7)
    RMIDDLE_CRUST = bcast_double_precision(8)
    RMOHO = bcast_double_precision(9)
    R80 = bcast_double_precision(10)
    R120 = bcast_double_precision(11)
    R220 = bcast_double_precision(12)
    R400 = bcast_double_precision(13)
    R600 = bcast_double_precision(14)
    R670 = bcast_double_precision(15)
    R771 = bcast_double_precision(16)
    RTOPDDOUBLEPRIME = bcast_double_precision(17)
    RCMB = bcast_double_precision(18)
    RICB = bcast_double_precision(19)
    R_CENTRAL_CUBE = bcast_double_precision(20)
    RHO_TOP_OC = bcast_double_precision(21)
    RHO_BOTTOM_OC = bcast_double_precision(22)
    RHO_OCEANS = bcast_double_precision(23)
    HDUR_MOVIE = bcast_double_precision(24)
    MOVIE_TOP = bcast_double_precision(25)
    MOVIE_BOTTOM = bcast_double_precision(26)
    MOVIE_WEST = bcast_double_precision(27)
    MOVIE_EAST = bcast_double_precision(28)
    MOVIE_NORTH = bcast_double_precision(29)
    MOVIE_SOUTH = bcast_double_precision(30)

  endif

! check simulation pararmeters
  if (SIMULATION_TYPE /= 1 .and.  SIMULATION_TYPE /= 2 .and. SIMULATION_TYPE /= 3) &
          call exit_MPI(myrank, 'SIMULATION_TYPE could be only 1, 2, or 3')

  if (SIMULATION_TYPE /= 1 .and. NSOURCES > 999999)  &
    call exit_MPI(myrank, 'for adjoint simulations, NSOURCES <= 999999, if you need more change i6.6 in write_seismograms.f90')

  if (ATTENUATION_VAL .or. SIMULATION_TYPE /= 1 .or. SAVE_FORWARD .or. (MOVIE_VOLUME .and. SIMULATION_TYPE /= 3)) then
    COMPUTE_AND_STORE_STRAIN = .true.
  else
    COMPUTE_AND_STORE_STRAIN = .false.
  endif

! get the base pathname for output files
  call get_value_string(OUTPUT_FILES, 'OUTPUT_FILES', 'OUTPUT_FILES')

! open main output file, only written to by process 0
  if(myrank == 0 .and. IMAIN /= ISTANDARD_OUTPUT) &
    open(unit=IMAIN,file=trim(OUTPUT_FILES)//'/output_solver.txt',status='unknown',action='write')

  if(myrank == 0) then

  write(IMAIN,*)
  write(IMAIN,*) '******************************'
  write(IMAIN,*) '**** Specfem3D MPI Solver ****'
  write(IMAIN,*) '******************************'
  write(IMAIN,*)
  write(IMAIN,*)

  if(FIX_UNDERFLOW_PROBLEM) write(IMAIN,*) 'Fixing slow underflow trapping problem using small initial field'

  write(IMAIN,*)
  write(IMAIN,*) 'There are ',sizeprocs,' MPI processes'
  write(IMAIN,*) 'Processes are numbered from 0 to ',sizeprocs-1
  write(IMAIN,*)

  write(IMAIN,*) 'There are ',NEX_XI,' elements along xi in each chunk'
  write(IMAIN,*) 'There are ',NEX_ETA,' elements along eta in each chunk'
  write(IMAIN,*)
  write(IMAIN,*) 'There are ',NPROC_XI,' slices along xi in each chunk'
  write(IMAIN,*) 'There are ',NPROC_ETA,' slices along eta in each chunk'
  write(IMAIN,*) 'There is a total of ',NPROC,' slices in each chunk'
  write(IMAIN,*) 'There are ',NCHUNKS,' chunks'
  write(IMAIN,*) 'There is a total of ',NPROCTOT,' slices in all the chunks'

  write(IMAIN,*)
  write(IMAIN,*) 'NDIM = ',NDIM
  write(IMAIN,*)
  write(IMAIN,*) 'NGLLX = ',NGLLX
  write(IMAIN,*) 'NGLLY = ',NGLLY
  write(IMAIN,*) 'NGLLZ = ',NGLLZ
  write(IMAIN,*)

! write information about precision used for floating-point operations
  if(CUSTOM_REAL == SIZE_REAL) then
    write(IMAIN,*) 'using single precision for the calculations'
  else
    write(IMAIN,*) 'using double precision for the calculations'
  endif
  write(IMAIN,*)
  write(IMAIN,*) 'smallest and largest possible floating-point numbers are: ',tiny(1._CUSTOM_REAL),huge(1._CUSTOM_REAL)
  write(IMAIN,*)

  endif

! check that the code is running with the requested nb of processes
  if(sizeprocs /= NPROCTOT) call exit_MPI(myrank,'wrong number of MPI processes')

! check that the code has been compiled with the right values
  if (NSPEC_computed(IREGION_CRUST_MANTLE) /= NSPEC_CRUST_MANTLE) then
      write(IMAIN,*) NSPEC_computed(IREGION_CRUST_MANTLE),NSPEC_CRUST_MANTLE
      call exit_MPI(myrank,'error in compiled parameters, please recompile solver 1')
  endif
  if (NSPEC_computed(IREGION_OUTER_CORE) /= NSPEC_OUTER_CORE) then
      write(IMAIN,*) NSPEC_computed(IREGION_OUTER_CORE),NSPEC_OUTER_CORE
       call exit_MPI(myrank,'error in compiled parameters, please recompile solver 2')
  endif
  if (NSPEC_computed(IREGION_INNER_CORE) /= NSPEC_INNER_CORE) then
      write(IMAIN,*) NSPEC_computed(IREGION_INNER_CORE),NSPEC_INNER_CORE
       call exit_MPI(myrank,'error in compiled parameters, please recompile solver 3')
  endif
  if (ATTENUATION_3D .NEQV. ATTENUATION_3D_VAL) then
      write(IMAIN,*) ATTENUATION_3D,ATTENUATION_3D_VAL
       call exit_MPI(myrank,'error in compiled parameters, please recompile solver 4')
  endif
  if (NCHUNKS /= NCHUNKS_VAL) then
      write(IMAIN,*) NCHUNKS,NCHUNKS_VAL
       call exit_MPI(myrank,'error in compiled parameters, please recompile solver 6')
  endif
  if (GRAVITY .NEQV. GRAVITY_VAL) then
      write(IMAIN,*) GRAVITY,GRAVITY_VAL
       call exit_MPI(myrank,'error in compiled parameters, please recompile solver 7')
  endif
  if (ROTATION .NEQV. ROTATION_VAL) then
      write(IMAIN,*) ROTATION,ROTATION_VAL
       call exit_MPI(myrank,'error in compiled parameters, please recompile solver 8')
  endif
  if (ATTENUATION .NEQV. ATTENUATION_VAL) then
      write(IMAIN,*) ATTENUATION,ATTENUATION_VAL
       call exit_MPI(myrank,'error in compiled parameters, please recompile solver 9')
  endif
  if (ELLIPTICITY .NEQV. ELLIPTICITY_VAL) then
      write(IMAIN,*) ELLIPTICITY,ELLIPTICITY_VAL
       call exit_MPI(myrank,'error in compiled parameters, please recompile solver 10')
  endif
  if (NPROCTOT /= NPROCTOT_VAL) then
      write(IMAIN,*) NPROCTOT,NPROCTOT_VAL
       call exit_MPI(myrank,'error in compiled parameters, please recompile solver 11')
  endif
  if (NEX_XI /= NEX_XI_VAL) then
      write(IMAIN,*) NEX_XI,NEX_XI_VAL
       call exit_MPI(myrank,'error in compiled parameters, please recompile solver 12')
  endif
  if (NEX_ETA /= NEX_ETA_VAL) then
      write(IMAIN,*) NEX_ETA,NEX_ETA_VAL
       call exit_MPI(myrank,'error in compiled parameters, please recompile solver 13')
  endif
  if (TRANSVERSE_ISOTROPY .NEQV. TRANSVERSE_ISOTROPY_VAL) then
      write(IMAIN,*) TRANSVERSE_ISOTROPY,TRANSVERSE_ISOTROPY_VAL
       call exit_MPI(myrank,'error in compiled parameters, please recompile solver 14')
  endif
  if (ANISOTROPIC_3D_MANTLE .NEQV. ANISOTROPIC_3D_MANTLE_VAL) then
      write(IMAIN,*) ANISOTROPIC_3D_MANTLE,ANISOTROPIC_3D_MANTLE_VAL
       call exit_MPI(myrank,'error in compiled parameters, please recompile solver 15')
  endif
  if (ANISOTROPIC_INNER_CORE .NEQV. ANISOTROPIC_INNER_CORE_VAL) then
      write(IMAIN,*) ANISOTROPIC_INNER_CORE,ANISOTROPIC_INNER_CORE_VAL
       call exit_MPI(myrank,'error in compiled parameters, please recompile solver 16')
  endif

! determine chunk number and local slice coordinates using addressing
  ichunk = ichunk_slice(myrank)
  iproc_xi = iproc_xi_slice(myrank)
  iproc_eta = iproc_eta_slice(myrank)

! make ellipticity
  if(ELLIPTICITY_VAL) call make_ellipticity(nspl,rspl,espl,espl2,ONE_CRUST)

! number of corners and faces shared between chunks and number of message types
  if(NCHUNKS_VAL == 1 .or. NCHUNKS_VAL == 2) then
    NCORNERSCHUNKS = 1
    NUM_FACES = 1
    NUM_MSG_TYPES = 1
  else if(NCHUNKS_VAL == 3) then
    NCORNERSCHUNKS = 1
    NUM_FACES = 1
    NUM_MSG_TYPES = 3
  else if(NCHUNKS_VAL == 6) then
    NCORNERSCHUNKS = 8
    NUM_FACES = 4
    NUM_MSG_TYPES = 3
  else
    call exit_MPI(myrank,'number of chunks must be either 1, 2, 3 or 6')
  endif

! if more than one chunk then same number of processors in each direction
  NPROC_ONE_DIRECTION = NPROC_XI

! total number of messages corresponding to these common faces
  NUMMSGS_FACES = NPROC_ONE_DIRECTION*NUM_FACES*NUM_MSG_TYPES

! check that the number of points in this slice is correct
  if(minval(ibool_crust_mantle) /= 1 .or. maxval(ibool_crust_mantle) /= NGLOB_CRUST_MANTLE) &
      call exit_MPI(myrank,'incorrect global numbering: iboolmax does not equal nglob in crust and mantle')

  if(minval(ibool_outer_core) /= 1 .or. maxval(ibool_outer_core) /= NGLOB_OUTER_CORE) &
    call exit_MPI(myrank,'incorrect global numbering: iboolmax does not equal nglob in outer core')

  if(minval(ibool_inner_core) /= 1 .or. maxval(ibool_inner_core) /= NGLOB_INNER_CORE) &
    call exit_MPI(myrank,'incorrect global numbering: iboolmax does not equal nglob in inner core')

! check that there is at least one receiver
  if(myrank == 0) then
    write(IMAIN,*)
    write(IMAIN,*) 'Total number of receivers = ', nrec
    write(IMAIN,*)
  endif
  if(nrec < 1) call exit_MPI(myrank,'need at least one receiver')

! $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

! locate sources in the mesh
  call locate_sources(NSOURCES,myrank,NSPEC_CRUST_MANTLE,NGLOB_CRUST_MANTLE,ibool_crust_mantle, &
            xstore_crust_mantle,ystore_crust_mantle,zstore_crust_mantle, &
            xigll,yigll,zigll,NPROCTOT,ELLIPTICITY,TOPOGRAPHY, &
            sec,t_cmt,yr,jda,ho,mi,theta_source,phi_source, &
            NSTEP,DT,hdur,Mxx,Myy,Mzz,Mxy,Mxz,Myz, &
            islice_selected_source,ispec_selected_source, &
            xi_source,eta_source,gamma_source, nu_source,&
            rspl,espl,espl2,nspl,ibathy_topo,NEX_XI,PRINT_SOURCE_TIME_FUNCTION)

  if(minval(t_cmt) /= 0.) call exit_MPI(myrank,'one t_cmt must be zero, others must be positive')

! convert the half duration for triangle STF to the one for gaussian STF
  hdur_gaussian = hdur/SOURCE_DECAY_MIMIC_TRIANGLE

! define t0 as the earliest start time
  t0 = - 1.5d0*minval(t_cmt-hdur)

! --------- receivers ---------------

  rec_filename = 'DATA/STATIONS'
  call get_value_string(STATIONS, 'solver.STATIONS', rec_filename)

! locate receivers in the crust in the mesh
  call locate_receivers(myrank,DT,NSTEP,NSPEC_CRUST_MANTLE,NGLOB_CRUST_MANTLE,ibool_crust_mantle, &
            xstore_crust_mantle,ystore_crust_mantle,zstore_crust_mantle, &
            xigll,yigll,zigll,trim(rec_filename), &
            nrec,islice_selected_rec,ispec_selected_rec, &
            xi_receiver,eta_receiver,gamma_receiver,station_name,network_name,stlat,stlon,stele,nu, &
            yr,jda,ho,mi,sec, &
            NPROCTOT,ELLIPTICITY,TOPOGRAPHY, &
            theta_source(1),phi_source(1),rspl,espl,espl2,nspl,ibathy_topo,RECEIVERS_CAN_BE_BURIED,NCHUNKS)

!$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

  do isource = 1,NSOURCES

!   check that the source slice number is okay
    if(islice_selected_source(isource) < 0 .or. islice_selected_source(isource) > NPROCTOT-1) &
      call exit_MPI(myrank,'something is wrong with the source slice number')

!   compute source arrays in source slice
    if(myrank == islice_selected_source(isource)) then
      call compute_arrays_source(ispec_selected_source(isource), &
             xi_source(isource),eta_source(isource),gamma_source(isource),sourcearray, &
             Mxx(isource),Myy(isource),Mzz(isource),Mxy(isource),Mxz(isource),Myz(isource), &
             xix_crust_mantle,xiy_crust_mantle,xiz_crust_mantle, &
             etax_crust_mantle,etay_crust_mantle,etaz_crust_mantle, &
             gammax_crust_mantle,gammay_crust_mantle,gammaz_crust_mantle, &
             xigll,yigll,zigll,NSPEC_CRUST_MANTLE)
      sourcearrays(isource,:,:,:,:) = sourcearray(:,:,:,:)
    endif
  enddo

!--- select local receivers

! count number of receivers located in this slice
  nrec_local = 0
    do irec = 1,nrec
      if(myrank == islice_selected_rec(irec)) nrec_local = nrec_local + 1
    enddo

  if (nrec_local > 0) then

! allocate Lagrange interpolators for receivers
  allocate(hxir_store(nrec_local,NGLLX),STAT=ier)
  if (ier /= 0 ) then
    print *,"ABORTING can not allocate in specfem3D ier=",ier
    call MPI_Abort(MPI_COMM_WORLD,errorcode,ier)
  endif

  allocate(hetar_store(nrec_local,NGLLY),STAT=ier)
  if (ier /= 0 ) then
    print *,"ABORTING can not allocate in specfem3D ier=",ier
    call MPI_Abort(MPI_COMM_WORLD,errorcode,ier)
  endif

  allocate(hgammar_store(nrec_local,NGLLZ),STAT=ier)
  if (ier /= 0 ) then
    print *,"ABORTING can not allocate in specfem3D ier=",ier
    call MPI_Abort(MPI_COMM_WORLD,errorcode,ier)
  endif

! define local to global receiver numbering mapping
  allocate(number_receiver_global(nrec_local),STAT=ier)
  if (ier /= 0 ) then
    print *,"ABORTING can not allocate in specfem3D ier=",ier
    call MPI_Abort(MPI_COMM_WORLD,errorcode,ier)
  endif

  irec_local = 0
    do irec = 1,nrec
      if(myrank == islice_selected_rec(irec)) then
        irec_local = irec_local + 1
        number_receiver_global(irec_local) = irec
      endif
    enddo

! define and store Lagrange interpolators at all the receivers
  do irec_local = 1,nrec_local
    irec = number_receiver_global(irec_local)
    call lagrange_any(xi_receiver(irec),NGLLX,xigll,hxir,hpxir)
    call lagrange_any(eta_receiver(irec),NGLLY,yigll,hetar,hpetar)
    call lagrange_any(gamma_receiver(irec),NGLLZ,zigll,hgammar,hpgammar)
    hxir_store(irec_local,:) = hxir(:)
    hetar_store(irec_local,:) = hetar(:)
    hgammar_store(irec_local,:) = hgammar(:)
  enddo

  endif ! nrec_local

! check that the sum of the number of receivers in each slice is nrec
  call MPI_REDUCE(nrec_local,nrec_tot_found,1,MPI_INTEGER,MPI_SUM,0,MPI_COMM_WORLD,ier)
  if(myrank == 0) then
    write(IMAIN,*)
    write(IMAIN,*) 'found a total of ',nrec_tot_found,' receivers in all the slices'
    if(nrec_tot_found /= nrec) then
      call exit_MPI(myrank,'problem when dispatching the receivers')
    else
      write(IMAIN,*) 'this total is okay'
    endif
  endif

  if(myrank == 0) then

  write(IMAIN,*)
  write(IMAIN,*) 'Total number of samples for seismograms = ',NSTEP
  write(IMAIN,*)

  write(IMAIN,*)
  write(IMAIN,*) 'Reference radius of the Earth used is ',R_EARTH_KM,' km'
  write(IMAIN,*)

  if(NSOURCES > 1) write(IMAIN,*) 'Using ',NSOURCES,' point sources'

  write(IMAIN,*)
  if(ELLIPTICITY_VAL) then
    write(IMAIN,*) 'incorporating ellipticity'
  else
    write(IMAIN,*) 'no ellipticity'
  endif

  write(IMAIN,*)
  if(TOPOGRAPHY) then
    write(IMAIN,*) 'incorporating surface topography'
  else
    write(IMAIN,*) 'no surface topography'
  endif

  write(IMAIN,*)
  if(ISOTROPIC_3D_MANTLE) then
    write(IMAIN,*) 'incorporating 3-D lateral variations'
  else
    write(IMAIN,*) 'no 3-D lateral variations'
  endif

  write(IMAIN,*)
  if(CRUSTAL) then
    write(IMAIN,*) 'incorporating crustal variations'
  else
    write(IMAIN,*) 'no crustal variations'
  endif

  write(IMAIN,*)
  if(ONE_CRUST) then
    write(IMAIN,*) 'using one layer only in PREM crust'
  else
    write(IMAIN,*) 'using unmodified 1D crustal model with two layers'
  endif

  write(IMAIN,*)
  if(GRAVITY_VAL) then
    write(IMAIN,*) 'incorporating self-gravitation (Cowling approximation)'
  else
    write(IMAIN,*) 'no self-gravitation'
  endif

  write(IMAIN,*)
  if(ROTATION_VAL) then
    write(IMAIN,*) 'incorporating rotation'
  else
    write(IMAIN,*) 'no rotation'
  endif

  write(IMAIN,*)
  if(TRANSVERSE_ISOTROPY_VAL) then
    write(IMAIN,*) 'incorporating transverse isotropy'
  else
    write(IMAIN,*) 'no transverse isotropy'
  endif

  write(IMAIN,*)
  if(ATTENUATION_VAL) then
    write(IMAIN,*) 'incorporating attenuation using ',N_SLS,' standard linear solids'
    if(ATTENUATION_3D_VAL) write(IMAIN,*) 'using 3D attenuation'
  else
    write(IMAIN,*) 'no attenuation'
  endif

  write(IMAIN,*)
  if(OCEANS) then
    write(IMAIN,*) 'incorporating the oceans using equivalent load'
  else
    write(IMAIN,*) 'no oceans'
  endif

  write(IMAIN,*)
  if(ANISOTROPIC_INNER_CORE_VAL) then
    write(IMAIN,*) 'incorporating anisotropic inner core'
  else
    write(IMAIN,*) 'no inner-core anisotropy'
  endif

  write(IMAIN,*)
  if(ANISOTROPIC_3D_MANTLE_VAL) then
    write(IMAIN,*) 'incorporating anisotropic mantle'
  else
    write(IMAIN,*) 'no general mantle anisotropy'
  endif
  write(IMAIN,*)
  write(IMAIN,*)
  write(IMAIN,*)

  endif

! the mass matrix needs to be assembled with MPI here once and for all

! ocean load
  if (OCEANS) then
    call assemble_MPI_scalar(myrank,rmass_ocean_load,NGLOB_CRUST_MANTLE, &
            iproc_xi,iproc_eta,ichunk,addressing, &
            iboolleft_xi_crust_mantle,iboolright_xi_crust_mantle,iboolleft_eta_crust_mantle,iboolright_eta_crust_mantle, &
            npoin2D_faces_crust_mantle,npoin2D_xi_crust_mantle,npoin2D_eta_crust_mantle, &
            iboolfaces_crust_mantle,iboolcorner_crust_mantle, &
            iprocfrom_faces,iprocto_faces,imsg_type, &
            iproc_master_corners,iproc_worker1_corners,iproc_worker2_corners, &
            buffer_send_faces,buffer_received_faces,npoin2D_max_all, &
            buffer_send_chunkcorners_scalar,buffer_recv_chunkcorners_scalar, &
            NUMMSGS_FACES,NUM_MSG_TYPES,NCORNERSCHUNKS, &
            NPROC_XI,NPROC_ETA,NGLOB1D_RADIAL(IREGION_CRUST_MANTLE), &
            NGLOB2DMAX_XMIN_XMAX(IREGION_CRUST_MANTLE),NGLOB2DMAX_YMIN_YMAX(IREGION_CRUST_MANTLE),NGLOB2DMAX_XY_VAL_CM,NCHUNKS)
  endif

! crust and mantle
  call assemble_MPI_scalar(myrank,rmass_crust_mantle,NGLOB_CRUST_MANTLE, &
            iproc_xi,iproc_eta,ichunk,addressing, &
            iboolleft_xi_crust_mantle,iboolright_xi_crust_mantle,iboolleft_eta_crust_mantle,iboolright_eta_crust_mantle, &
            npoin2D_faces_crust_mantle,npoin2D_xi_crust_mantle,npoin2D_eta_crust_mantle, &
            iboolfaces_crust_mantle,iboolcorner_crust_mantle, &
            iprocfrom_faces,iprocto_faces,imsg_type, &
            iproc_master_corners,iproc_worker1_corners,iproc_worker2_corners, &
            buffer_send_faces,buffer_received_faces,npoin2D_max_all, &
            buffer_send_chunkcorners_scalar,buffer_recv_chunkcorners_scalar, &
            NUMMSGS_FACES,NUM_MSG_TYPES,NCORNERSCHUNKS, &
            NPROC_XI,NPROC_ETA,NGLOB1D_RADIAL(IREGION_CRUST_MANTLE), &
            NGLOB2DMAX_XMIN_XMAX(IREGION_CRUST_MANTLE),NGLOB2DMAX_YMIN_YMAX(IREGION_CRUST_MANTLE),NGLOB2DMAX_XY_VAL_CM,NCHUNKS)

! outer core
  call assemble_MPI_scalar(myrank,rmass_outer_core,NGLOB_OUTER_CORE, &
            iproc_xi,iproc_eta,ichunk,addressing, &
            iboolleft_xi_outer_core,iboolright_xi_outer_core,iboolleft_eta_outer_core,iboolright_eta_outer_core, &
            npoin2D_faces_outer_core,npoin2D_xi_outer_core,npoin2D_eta_outer_core, &
            iboolfaces_outer_core,iboolcorner_outer_core, &
            iprocfrom_faces,iprocto_faces,imsg_type, &
            iproc_master_corners,iproc_worker1_corners,iproc_worker2_corners, &
            buffer_send_faces,buffer_received_faces,npoin2D_max_all, &
            buffer_send_chunkcorners_scalar,buffer_recv_chunkcorners_scalar, &
            NUMMSGS_FACES,NUM_MSG_TYPES,NCORNERSCHUNKS, &
            NPROC_XI,NPROC_ETA,NGLOB1D_RADIAL(IREGION_OUTER_CORE), &
            NGLOB2DMAX_XMIN_XMAX(IREGION_OUTER_CORE),NGLOB2DMAX_YMIN_YMAX(IREGION_OUTER_CORE),NGLOB2DMAX_XY_VAL_OC,NCHUNKS)

! inner core
  call assemble_MPI_scalar(myrank,rmass_inner_core,NGLOB_INNER_CORE, &
            iproc_xi,iproc_eta,ichunk,addressing, &
            iboolleft_xi_inner_core,iboolright_xi_inner_core,iboolleft_eta_inner_core,iboolright_eta_inner_core, &
            npoin2D_faces_inner_core,npoin2D_xi_inner_core,npoin2D_eta_inner_core, &
            iboolfaces_inner_core,iboolcorner_inner_core, &
            iprocfrom_faces,iprocto_faces,imsg_type, &
            iproc_master_corners,iproc_worker1_corners,iproc_worker2_corners, &
            buffer_send_faces,buffer_received_faces,npoin2D_max_all, &
            buffer_send_chunkcorners_scalar,buffer_recv_chunkcorners_scalar, &
            NUMMSGS_FACES,NUM_MSG_TYPES,NCORNERSCHUNKS, &
            NPROC_XI,NPROC_ETA,NGLOB1D_RADIAL(IREGION_INNER_CORE), &
            NGLOB2DMAX_XMIN_XMAX(IREGION_INNER_CORE),NGLOB2DMAX_YMIN_YMAX(IREGION_INNER_CORE),NGLOB2DMAX_XY_VAL_IC,NCHUNKS)

  if(myrank == 0) write(IMAIN,*) 'end assembling MPI mass matrix'

!
!--- handle the communications with the central cube if it was included in the mesh
!
  if(INCLUDE_CENTRAL_CUBE) then

    if(myrank == 0) write(IMAIN,*) 'including central cube'

! compute number of messages to expect in cube as well as their size
    call comp_central_cube_buffer_size(iproc_xi,iproc_eta,ichunk,NPROC_XI,NPROC_ETA,NSPEC2D_BOTTOM(IREGION_INNER_CORE), &
                nb_msgs_theor_in_cube,npoin2D_cube_from_slices)

! this value is used for dynamic memory allocation, therefore make sure it is never zero
    if(nb_msgs_theor_in_cube > 0) then
      non_zero_nb_msgs_theor_in_cube = nb_msgs_theor_in_cube
    else
      non_zero_nb_msgs_theor_in_cube = 1
    endif

! allocate buffers for cube and slices
    allocate(sender_from_slices_to_cube(non_zero_nb_msgs_theor_in_cube),STAT=ier)
    if (ier /= 0 ) then
      print *,"ABORTING can not allocate in specfem3D ier=",ier
      call MPI_Abort(MPI_COMM_WORLD,errorcode,ier)
    endif

    allocate(buffer_all_cube_from_slices(non_zero_nb_msgs_theor_in_cube,npoin2D_cube_from_slices,NDIM),STAT=ier)
    if (ier /= 0 ) then
      print *,"ABORTING can not allocate in specfem3D ier=",ier
      call MPI_Abort(MPI_COMM_WORLD,errorcode,ier)
    endif

    allocate(buffer_slices(npoin2D_cube_from_slices,NDIM),STAT=ier)
    if (ier /= 0 ) then
      print *,"ABORTING can not allocate in specfem3D ier=",ier
      call MPI_Abort(MPI_COMM_WORLD,errorcode,ier)
    endif

    allocate(buffer_slices2(npoin2D_cube_from_slices,NDIM),STAT=ier)
    if (ier /= 0 ) then
      print *,"ABORTING can not allocate in specfem3D ier=",ier
      call MPI_Abort(MPI_COMM_WORLD,errorcode,ier)
    endif

    allocate(ibool_central_cube(non_zero_nb_msgs_theor_in_cube,npoin2D_cube_from_slices),STAT=ier)
    if (ier /= 0 ) then
      print *,"ABORTING can not allocate in specfem3D ier=",ier
      call MPI_Abort(MPI_COMM_WORLD,errorcode,ier)
    endif

! create buffers to assemble with the central cube
    call create_central_cube_buffers(myrank,iproc_xi,iproc_eta,ichunk, &
       NPROC_XI,NPROC_ETA,NCHUNKS,NSPEC_INNER_CORE,NGLOB_INNER_CORE, &
       NSPEC2DMAX_XMIN_XMAX(IREGION_INNER_CORE),NSPEC2DMAX_YMIN_YMAX(IREGION_INNER_CORE), &
       NSPEC2D_BOTTOM(IREGION_INNER_CORE), &
       addressing,ibool_inner_core,idoubling_inner_core, &
       xstore_inner_core,ystore_inner_core,zstore_inner_core, &
       nspec2D_xmin_inner_core,nspec2D_xmax_inner_core,nspec2D_ymin_inner_core,nspec2D_ymax_inner_core, &
       ibelm_xmin_inner_core,ibelm_xmax_inner_core,ibelm_ymin_inner_core,ibelm_ymax_inner_core,ibelm_bottom_inner_core, &
       nb_msgs_theor_in_cube,non_zero_nb_msgs_theor_in_cube,npoin2D_cube_from_slices, &
       receiver_cube_from_slices,sender_from_slices_to_cube,ibool_central_cube, &
       buffer_slices,buffer_slices2,buffer_all_cube_from_slices)

    if(myrank == 0) write(IMAIN,*) 'done including central cube'

! the mass matrix to assemble is a scalar, not a vector
    ndim_assemble = 1

! use these buffers to assemble the inner core mass matrix with the central cube
    call assemble_MPI_central_cube(ichunk,nb_msgs_theor_in_cube, sender_from_slices_to_cube, &
     npoin2D_cube_from_slices, buffer_all_cube_from_slices, buffer_slices, buffer_slices2, ibool_central_cube, &
     receiver_cube_from_slices, ibool_inner_core, idoubling_inner_core, NSPEC_INNER_CORE, &
     ibelm_bottom_inner_core, NSPEC2D_BOTTOM(IREGION_INNER_CORE),NGLOB_INNER_CORE,rmass_inner_core,ndim_assemble)

! suppress fictitious mass matrix elements in central cube
! because the slices do not compute all their spectral elements in the cube
    where(rmass_inner_core(:) <= 0.) rmass_inner_core = 1.

  endif   ! end of handling the communications with the central cube

! check that all the mass matrices are positive
  if(minval(rmass_crust_mantle) <= 0. .or. minval(rmass_inner_core) <= 0. .or. minval(rmass_outer_core) <= 0.) &
       call exit_MPI(myrank,'negative mass matrix term for at least one region')

  if(OCEANS) then
    if(minval(rmass_ocean_load) <= 0.) call exit_MPI(myrank,'negative mass matrix term for the oceans')
  endif

! for efficiency, invert final mass matrix once and for all on each slice
  if(OCEANS) rmass_ocean_load = 1._CUSTOM_REAL / rmass_ocean_load
  rmass_crust_mantle = 1._CUSTOM_REAL / rmass_crust_mantle
  rmass_outer_core = 1._CUSTOM_REAL / rmass_outer_core
  rmass_inner_core = 1._CUSTOM_REAL / rmass_inner_core

! change x, y, z to r, theta and phi once and for all
! IMPROVE dangerous: old name kept (xstore ystore zstore) for new values

! convert in the crust and mantle
    do iglob = 1,NGLOB_CRUST_MANTLE
      call xyz_2_rthetaphi(xstore_crust_mantle(iglob), &
    ystore_crust_mantle(iglob),zstore_crust_mantle(iglob),rval,thetaval,phival)
      xstore_crust_mantle(iglob) = rval
      ystore_crust_mantle(iglob) = thetaval
      zstore_crust_mantle(iglob) = phival
    enddo

! convert in the outer core
    do iglob = 1,NGLOB_OUTER_CORE
      call xyz_2_rthetaphi(xstore_outer_core(iglob), &
    ystore_outer_core(iglob),zstore_outer_core(iglob),rval,thetaval,phival)
      xstore_outer_core(iglob) = rval
      ystore_outer_core(iglob) = thetaval
      zstore_outer_core(iglob) = phival
    enddo

! convert in the inner core
    do iglob = 1,NGLOB_INNER_CORE
      call xyz_2_rthetaphi(xstore_inner_core(iglob), &
    ystore_inner_core(iglob),zstore_inner_core(iglob),rval,thetaval,phival)
      xstore_inner_core(iglob) = rval
      ystore_inner_core(iglob) = thetaval
      zstore_inner_core(iglob) = phival
    enddo

  if(ATTENUATION_VAL) then

! get and store PREM attenuation model

        call get_attenuation_model_1D(myrank, IREGION_CRUST_MANTLE, tau_sigma_dble, &
             omsb_crust_mantle_dble, factor_common_crust_mantle_dble,  &
             factor_scale_crust_mantle_dble, NRAD_ATTENUATION,1,1,1, AM_V)
        omsb_inner_core_dble(:,:,:,1:min(ATT4,ATT5)) = omsb_crust_mantle_dble(:,:,:,1:min(ATT4,ATT5))
        factor_scale_inner_core_dble(:,:,:,1:min(ATT4,ATT5))    = factor_scale_crust_mantle_dble(:,:,:,1:min(ATT4,ATT5))
        factor_common_inner_core_dble(:,:,:,:,1:min(ATT4,ATT5)) = factor_common_crust_mantle_dble(:,:,:,:,1:min(ATT4,ATT5))
        ! Tell the Attenuation Code about the IDOUBLING regions within the Mesh
        call set_attenuation_regions_1D(RICB, RCMB, R670, R220, R80, AM_V)

   if(CUSTOM_REAL == SIZE_REAL) then
      factor_scale_crust_mantle       = sngl(factor_scale_crust_mantle_dble)
      one_minus_sum_beta_crust_mantle = sngl(omsb_crust_mantle_dble)
      factor_common_crust_mantle      = sngl(factor_common_crust_mantle_dble)

      factor_scale_inner_core         = sngl(factor_scale_inner_core_dble)
      one_minus_sum_beta_inner_core   = sngl(omsb_inner_core_dble)
      factor_common_inner_core        = sngl(factor_common_inner_core_dble)
   else
      factor_scale_crust_mantle       = factor_scale_crust_mantle_dble
      one_minus_sum_beta_crust_mantle = omsb_crust_mantle_dble
      factor_common_crust_mantle      = factor_common_crust_mantle_dble

      factor_scale_inner_core         = factor_scale_inner_core_dble
      one_minus_sum_beta_inner_core   = omsb_inner_core_dble
      factor_common_inner_core        = factor_common_inner_core_dble
   endif

! if attenuation is on, shift PREM to right frequency
! rescale mu in PREM to average frequency for attenuation
! the formulas to implement the scaling can be found for instance in
! Liu, H. P., Anderson, D. L. and Kanamori, H., Velocity dispersion due to
! anelasticity: implications for seismology and mantle composition,
! Geophys. J. R. Astron. Soc., vol. 47, pp. 41-58 (1976)
! and in Aki, K. and Richards, P. G., Quantitative seismology, theory and methods,
! W. H. Freeman, (1980), second edition, sections 5.5 and 5.5.2, eq. (5.81) p. 170

! rescale in crust and mantle

    do ispec = 1,NSPEC_CRUST_MANTLE
      do k=1,NGLLZ
        do j=1,NGLLY
          do i=1,NGLLX

! ATTENUATION_3D get scale_factor
            if(ATTENUATION_3D_VAL) then
              ! tau_mu and tau_sigma need to reference a point in the mesh
              scale_factor = factor_scale_crust_mantle(i,j,k,ispec)
            else
              iglob   = ibool_crust_mantle(i,j,k,ispec)
              dist_cr = xstore_crust_mantle(iglob)
              call get_attenuation_index(idoubling_crust_mantle(ispec), dble(dist_cr), iregion_selected, .FALSE., AM_V)
              scale_factor = factor_scale_crust_mantle(1,1,1,iregion_selected)
            endif ! ATTENUATION_3D

    if(ANISOTROPIC_3D_MANTLE_VAL) then
      scale_factor_minus_one = scale_factor - 1.
      mul = c44store_crust_mantle(i,j,k,ispec)
      c11store_crust_mantle(i,j,k,ispec) = c11store_crust_mantle(i,j,k,ispec) &
              + FOUR_THIRDS * scale_factor_minus_one * mul
      c12store_crust_mantle(i,j,k,ispec) = c12store_crust_mantle(i,j,k,ispec) &
              - TWO_THIRDS * scale_factor_minus_one * mul
      c13store_crust_mantle(i,j,k,ispec) = c13store_crust_mantle(i,j,k,ispec) &
              - TWO_THIRDS * scale_factor_minus_one * mul
      c22store_crust_mantle(i,j,k,ispec) = c22store_crust_mantle(i,j,k,ispec) &
              + FOUR_THIRDS * scale_factor_minus_one * mul
      c23store_crust_mantle(i,j,k,ispec) = c23store_crust_mantle(i,j,k,ispec) &
              - TWO_THIRDS * scale_factor_minus_one * mul
      c33store_crust_mantle(i,j,k,ispec) = c33store_crust_mantle(i,j,k,ispec) &
              + FOUR_THIRDS * scale_factor_minus_one * mul
      c44store_crust_mantle(i,j,k,ispec) = c44store_crust_mantle(i,j,k,ispec) &
              + scale_factor_minus_one * mul
      c55store_crust_mantle(i,j,k,ispec) = c55store_crust_mantle(i,j,k,ispec) &
              + scale_factor_minus_one * mul
      c66store_crust_mantle(i,j,k,ispec) = c66store_crust_mantle(i,j,k,ispec) &
              + scale_factor_minus_one * mul
    else
      muvstore_crust_mantle(i,j,k,ispec) = muvstore_crust_mantle(i,j,k,ispec) * scale_factor
      if(TRANSVERSE_ISOTROPY_VAL .and. (idoubling_crust_mantle(ispec) == IFLAG_220_80 &
      .or. idoubling_crust_mantle(ispec) == IFLAG_80_MOHO)) &
        muhstore_crust_mantle(i,j,k,ispec) = muhstore_crust_mantle(i,j,k,ispec) * scale_factor
    endif

          enddo
        enddo
      enddo
    enddo ! END DO CRUST MANTLE

! rescale in inner core

    do ispec = 1,NSPEC_INNER_CORE
      do k=1,NGLLZ
        do j=1,NGLLY
          do i=1,NGLLX

            if(ATTENUATION_3D_VAL) then
               scale_factor_minus_one = factor_scale_inner_core(i,j,k,ispec) - 1.0
            else
               iglob   = ibool_inner_core(i,j,k,ispec)
               dist_cr = xstore_inner_core(iglob)
               call get_attenuation_index(idoubling_inner_core(ispec), dble(dist_cr), iregion_selected, .TRUE., AM_V)
               scale_factor_minus_one = factor_scale_inner_core(1,1,1,iregion_selected) - 1.
            endif

        if(ANISOTROPIC_INNER_CORE_VAL) then
          mul = muvstore_inner_core(i,j,k,ispec)
          c11store_inner_core(i,j,k,ispec) = c11store_inner_core(i,j,k,ispec) &
                  + FOUR_THIRDS * scale_factor_minus_one * mul
          c12store_inner_core(i,j,k,ispec) = c12store_inner_core(i,j,k,ispec) &
                  - TWO_THIRDS * scale_factor_minus_one * mul
          c13store_inner_core(i,j,k,ispec) = c13store_inner_core(i,j,k,ispec) &
                  - TWO_THIRDS * scale_factor_minus_one * mul
          c33store_inner_core(i,j,k,ispec) = c33store_inner_core(i,j,k,ispec) &
                  + FOUR_THIRDS * scale_factor_minus_one * mul
          c44store_inner_core(i,j,k,ispec) = c44store_inner_core(i,j,k,ispec) &
                  + scale_factor_minus_one * mul
        endif

            if(ATTENUATION_3D_VAL) then
               muvstore_inner_core(i,j,k,ispec) = muvstore_inner_core(i,j,k,ispec) * factor_scale_inner_core(i,j,k,ispec)
            else
               muvstore_inner_core(i,j,k,ispec) = muvstore_inner_core(i,j,k,ispec) * factor_scale_inner_core(1,1,1,iregion_selected)
            endif

          enddo
        enddo
      enddo
    enddo ! END DO INNER CORE

  endif ! END IF(ATTENUATION)

! allocate seismogram array
  if (nrec_local > 0) then
      allocate(seismograms(NDIM,nrec_local,NTSTEP_BETWEEN_OUTPUT_SEISMOS),stat=ier)
      if (ier /= 0 ) then
        print *,"ABORTING can not allocate in specfem3D while allocating seismograms ier=",ier
        call MPI_Abort(MPI_COMM_WORLD,errorcode,ier)
      endif
! initialize seismograms
    seismograms(:,:,:) = 0._CUSTOM_REAL
    nit_written = 0
  endif

! initialize arrays to zero

  displ_crust_mantle(:,:) = 0._CUSTOM_REAL
  veloc_crust_mantle(:,:) = 0._CUSTOM_REAL
  accel_crust_mantle(:,:) = 0._CUSTOM_REAL

  displ_outer_core(:) = 0._CUSTOM_REAL
  veloc_outer_core(:) = 0._CUSTOM_REAL
  accel_outer_core(:) = 0._CUSTOM_REAL

  displ_inner_core(:,:) = 0._CUSTOM_REAL
  veloc_inner_core(:,:) = 0._CUSTOM_REAL
  accel_inner_core(:,:) = 0._CUSTOM_REAL

! put negligible initial value to avoid very slow underflow trapping
  if(FIX_UNDERFLOW_PROBLEM) then
    displ_crust_mantle(:,:) = VERYSMALLVAL
    displ_outer_core(:) = VERYSMALLVAL
    displ_inner_core(:,:) = VERYSMALLVAL
  endif

! store g, rho and dg/dr=dg using normalized radius in lookup table every 100 m
! get density and velocity from PREM model using dummy doubling flag
! this assumes that the gravity perturbations are small and smooth
! and that we can neglect the 3D model and use PREM every 100 m in all cases
! this is probably a rather reasonable assumption

 ! tabulate d ln(rho)/dr needed for the no gravity fluid potential
     do int_radius = 1,NRAD_GRAVITY
       radius = dble(int_radius) / (R_EARTH_KM * 10.d0)
       idoubling = 0
       call prem_iso(myrank,radius,rho,drhodr,vp,vs,Qkappa,Qmu,idoubling,.false., &
           ONE_CRUST,.false.,RICB,RCMB,RTOPDDOUBLEPRIME, &
           R600,R670,R220,R771,R400,R80,RMOHO,RMIDDLE_CRUST,ROCEAN)
       d_ln_density_dr_table(int_radius) = drhodr/rho
     enddo

  if(myrank == 0) then
    write(IMAIN,*)
    write(IMAIN,*) '           time step: ',sngl(DT),' s'
    write(IMAIN,*) 'number of time steps: ',NSTEP
    write(IMAIN,*) 'total simulated time: ',sngl(((NSTEP-1)*DT-t0)/60.d0),' minutes'
    write(IMAIN,*) 'start time:',sngl(-t0),' seconds'
    write(IMAIN,*)
  endif

! define constants for the time integration
! scaling to make displacement in meters and velocity in meters per second
  scale_t = ONE/dsqrt(PI*GRAV*RHOAV)
  scale_displ = R_EARTH
  scale_veloc = scale_displ / scale_t

! distinguish between single and double precision for reals
  if(CUSTOM_REAL == SIZE_REAL) then
    deltat = sngl(DT/scale_t)
  else
    deltat = DT/scale_t
  endif
  deltatover2 = 0.5d0*deltat
  deltatsqover2 = 0.5d0*deltat*deltat

! precompute Runge-Kutta coefficients if attenuation
  if(ATTENUATION_VAL) then
     call attenuation_memory_values(tau_sigma_dble, deltat, alphaval_dble, betaval_dble, gammaval_dble)
     if(CUSTOM_REAL == SIZE_REAL) then
        alphaval = sngl(alphaval_dble)
        betaval  = sngl(betaval_dble)
        gammaval = sngl(gammaval_dble)
     else
        alphaval = alphaval_dble
        betaval  = betaval_dble
        gammaval = gammaval_dble
     endif
  endif

  if (COMPUTE_AND_STORE_STRAIN) then
    epsilondev_crust_mantle(:,:,:,:,:) = 0._CUSTOM_REAL
    epsilondev_inner_core(:,:,:,:,:) = 0._CUSTOM_REAL
    if(FIX_UNDERFLOW_PROBLEM) then
      epsilondev_crust_mantle(:,:,:,:,:) = VERYSMALLVAL
      epsilondev_inner_core(:,:,:,:,:) = VERYSMALLVAL
    endif
  endif

! clear memory variables if attenuation
  if(ATTENUATION_VAL) then
    if (NSPEC_CRUST_MANTLE_ATTENUAT /= NSPEC_CRUST_MANTLE) &
       call exit_MPI(myrank, 'NSPEC_CRUST_MANTLE_ATTENUAT /= NSPEC_CRUST_MANTLE, exit')
    if (NSPEC_INNER_CORE_ATTENUATION /= NSPEC_INNER_CORE) &
       call exit_MPI(myrank, 'NSPEC_INNER_CORE_ATTENUATION /= NSPEC_INNER_CORE, exit')

    R_memory_crust_mantle(:,:,:,:,:,:) = 0._CUSTOM_REAL
    R_memory_inner_core(:,:,:,:,:,:) = 0._CUSTOM_REAL
    if(FIX_UNDERFLOW_PROBLEM) then
      R_memory_crust_mantle(:,:,:,:,:,:) = VERYSMALLVAL
      R_memory_inner_core(:,:,:,:,:,:) = VERYSMALLVAL
    endif

  endif

! get information about event name and location for SAC seismograms
  call get_event_info_parallel(myrank,yr_SAC,jda_SAC,ho_SAC,mi_SAC,sec_SAC,t_cmt_SAC, &
                 elat_SAC,elon_SAC,depth_SAC,mb_SAC,ename_SAC,cmt_lat_SAC,cmt_lon_SAC,cmt_depth_SAC,cmt_hdur_SAC,NSOURCES_SAC)

! define correct time steps if restart files
  if(NUMBER_OF_RUNS < 1 .or. NUMBER_OF_RUNS > 3) stop 'number of restart runs can be 1, 2 or 3'
  if(NUMBER_OF_THIS_RUN < 1 .or. NUMBER_OF_THIS_RUN > NUMBER_OF_RUNS) stop 'incorrect run number'
  if (SIMULATION_TYPE /= 1 .and. NUMBER_OF_RUNS /= 1) stop 'Only 1 run for SIMULATION_TYPE = 2/3'

    it_begin = 1
    it_end = NSTEP

!
!   s t a r t   t i m e   i t e r a t i o n s
!

! synchronize all processes to make sure everybody is ready to start time loop
  call MPI_BARRIER(MPI_COMM_WORLD,ier)
  if(myrank == 0) write(IMAIN,*) 'All processes are synchronized before time loop'

  if(myrank == 0) then
    write(IMAIN,*)
    write(IMAIN,*) 'Starting time iteration loop...'
    write(IMAIN,*)
  endif

! create an empty file to monitor the start of the simulation
  if(myrank == 0) then
    open(unit=IOUT,file=trim(OUTPUT_FILES)//'/starttimeloop.txt',status='unknown',action='write')
    write(IOUT,*) 'hello, starting time loop'
    close(IOUT)
  endif

! get MPI starting time
  time_start = MPI_WTIME()

! initialize variables for writing seismograms
  seismo_offset = it_begin-1
  seismo_current = 0

! *********************************************************
! ************* MAIN LOOP OVER THE TIME STEPS *************
! *********************************************************

  do it = it_begin,it_end

! update position in seismograms
    seismo_current = seismo_current + 1

! mantle
  do i=1,NGLOB_CRUST_MANTLE
    displ_crust_mantle(:,i) = displ_crust_mantle(:,i) + deltat*veloc_crust_mantle(:,i) + deltatsqover2*accel_crust_mantle(:,i)
    veloc_crust_mantle(:,i) = veloc_crust_mantle(:,i) + deltatover2*accel_crust_mantle(:,i)
  enddo

! outer core
  do i=1,NGLOB_OUTER_CORE
    displ_outer_core(i) = displ_outer_core(i) + deltat*veloc_outer_core(i) + deltatsqover2*accel_outer_core(i)
    veloc_outer_core(i) = veloc_outer_core(i) + deltatover2*accel_outer_core(i)
  enddo

! inner core
  do i=1,NGLOB_INNER_CORE
    displ_inner_core(:,i) = displ_inner_core(:,i) + deltat*veloc_inner_core(:,i) + deltatsqover2*accel_inner_core(:,i)
    veloc_inner_core(:,i) = veloc_inner_core(:,i) + deltatover2*accel_inner_core(:,i)
  enddo

! compute the maximum of the norm of the displacement
! in all the slices using an MPI reduction
! and output timestamp file to check that simulation is running fine
  if(mod(it,NTSTEP_BETWEEN_OUTPUT_INFO) == 0 .or. it == 5 .or. it == NSTEP) then

! compute maximum of norm of displacement in each slice
    Usolidnorm = max( &
        maxval(sqrt(displ_crust_mantle(1,:)**2 + &
          displ_crust_mantle(2,:)**2 + displ_crust_mantle(3,:)**2)), &
        maxval(sqrt(displ_inner_core(1,:)**2 + displ_inner_core(2,:)**2 + displ_inner_core(3,:)**2)))

    Ufluidnorm = maxval(abs(displ_outer_core))

! compute the maximum of the maxima for all the slices using an MPI reduction
    call MPI_REDUCE(Usolidnorm,Usolidnorm_all,1,CUSTOM_MPI_TYPE,MPI_MAX,0, &
                          MPI_COMM_WORLD,ier)
    call MPI_REDUCE(Ufluidnorm,Ufluidnorm_all,1,CUSTOM_MPI_TYPE,MPI_MAX,0, &
                          MPI_COMM_WORLD,ier)

    if(myrank == 0) then

      write(IMAIN,*) 'Time step # ',it
      write(IMAIN,*) 'Time: ',sngl(((it-1)*DT-t0)/60.d0),' minutes'

! rescale maximum displacement to correct dimensions
      Usolidnorm_all = Usolidnorm_all * sngl(scale_displ)
      write(IMAIN,*) 'Max norm displacement vector U in solid in all slices (m) = ',Usolidnorm_all
      write(IMAIN,*) 'Max non-dimensional potential Ufluid in fluid in all slices = ',Ufluidnorm_all

! elapsed time since beginning of the simulation
      tCPU = MPI_WTIME() - time_start
      int_tCPU = int(tCPU)
      ihours = int_tCPU / 3600
      iminutes = (int_tCPU - 3600*ihours) / 60
      iseconds = int_tCPU - 3600*ihours - 60*iminutes
      write(IMAIN,*) 'Elapsed time in seconds = ',tCPU
      write(IMAIN,"(' Elapsed time in hh:mm:ss = ',i4,' h ',i2.2,' m ',i2.2,' s')") ihours,iminutes,iseconds
      write(IMAIN,*) 'Mean elapsed time per time step in seconds = ',tCPU/dble(it)

! compute estimated remaining simulation time
      t_remain = (NSTEP - it) * (tCPU/dble(it))
      int_t_remain = int(t_remain)
      ihours_remain = int_t_remain / 3600
      iminutes_remain = (int_t_remain - 3600*ihours_remain) / 60
      iseconds_remain = int_t_remain - 3600*ihours_remain - 60*iminutes_remain
      write(IMAIN,*) 'Time steps done = ',it,' out of ',NSTEP
      write(IMAIN,*) 'Time steps remaining = ',NSTEP - it
      write(IMAIN,*) 'Estimated remaining time in seconds = ',t_remain
      write(IMAIN,"(' Estimated remaining time in hh:mm:ss = ',i4,' h ',i2.2,' m ',i2.2,' s')") &
               ihours_remain,iminutes_remain,iseconds_remain

! compute estimated total simulation time
      t_total = t_remain + tCPU
      int_t_total = int(t_total)
      ihours_total = int_t_total / 3600
      iminutes_total = (int_t_total - 3600*ihours_total) / 60
      iseconds_total = int_t_total - 3600*ihours_total - 60*iminutes_total
      write(IMAIN,*) 'Estimated total run time in seconds = ',t_total
      write(IMAIN,"(' Estimated total run time in hh:mm:ss = ',i4,' h ',i2.2,' m ',i2.2,' s')") &
               ihours_total,iminutes_total,iseconds_total
      write(IMAIN,*) 'We have done ',sngl(100.d0*dble(it)/dble(NSTEP)),'% of that'

      if(it < 100) then
        write(IMAIN,*) '************************************************************'
        write(IMAIN,*) '**** BEWARE: the above time estimates are not reliable'
        write(IMAIN,*) '**** because fewer than 100 iterations have been performed'
        write(IMAIN,*) '************************************************************'
      endif

      if(it < NSTEP) then

! get current date
      call date_and_time(datein,timein,zone,time_values)
! time_values(1): year
! time_values(2): month of the year
! time_values(3): day of the month
! time_values(5): hour of the day
! time_values(6): minutes of the hour

! compute date at which the run should finish; for simplicity only minutes
! are considered, seconds are ignored; in any case the prediction is not
! accurate down to seconds because of system and network fluctuations
      year = time_values(1)
      mon = time_values(2)
      day = time_values(3)
      hr = time_values(5)
      minutes = time_values(6)

! get timestamp in minutes of current date and time
      call convtime(timestamp,year,mon,day,hr,minutes)

! add remaining minutes
      timestamp = timestamp + nint(t_remain / 60.d0)

! get date and time of that future timestamp in minutes
      call invtime(timestamp,year,mon,day,hr,minutes)

! convert to Julian day to get day of the week
      call calndr(day,mon,year,julian_day_number)
      day_of_week = idaywk(julian_day_number)

      write(IMAIN,"(' The run will finish approximately on (in local time): ',a3,' ',a3,' ',i2.2,', ',i4.4,' ',i2.2,':',i2.2)") &
          weekday_name(day_of_week),month_name(mon),day,year,hr,minutes

! print date and time estimate of end of run in another country.
! For instance: the code runs at Caltech in California but the person
! running the code is connected remotely from France, which has 9 hours more
      if(ADD_TIME_ESTIMATE_ELSEWHERE .and. HOURS_TIME_DIFFERENCE * 60 + MINUTES_TIME_DIFFERENCE /= 0) then

! add time difference with that remote location (can be negative)
        timestamp_remote = timestamp + HOURS_TIME_DIFFERENCE * 60 + MINUTES_TIME_DIFFERENCE

! get date and time of that future timestamp in minutes
        call invtime(timestamp_remote,year_remote,mon_remote,day_remote,hr_remote,minutes_remote)

! convert to Julian day to get day of the week
        call calndr(day_remote,mon_remote,year_remote,julian_day_number)
        day_of_week_remote = idaywk(julian_day_number)

        if(HOURS_TIME_DIFFERENCE * 60 + MINUTES_TIME_DIFFERENCE > 0) then
          write(IMAIN,*) 'Adding positive time difference of ',abs(HOURS_TIME_DIFFERENCE),' hours'
        else
          write(IMAIN,*) 'Adding negative time difference of ',abs(HOURS_TIME_DIFFERENCE),' hours'
        endif
        write(IMAIN,*) 'and ',abs(MINUTES_TIME_DIFFERENCE),' minutes to get estimate at a remote location'
        write(IMAIN, &
            "(' The run will finish approximately on: ',a3,' ',a3,' ',i2.2,', ',i4.4,' ',i2.2,':',i2.2)") &
            weekday_name(day_of_week_remote),month_name(mon_remote),day_remote,year_remote,hr_remote,minutes_remote
      endif

      if(it < 100) then
        write(IMAIN,*) '************************************************************'
        write(IMAIN,*) '**** BEWARE: the above time estimates are not reliable'
        write(IMAIN,*) '**** because fewer than 100 iterations have been performed'
        write(IMAIN,*) '************************************************************'
      endif

      endif

      write(IMAIN,*)

! write time stamp file to give information about progression of simulation
      write(outputname,"('/timestamp',i6.6)") it

      open(unit=IOUT,file=trim(OUTPUT_FILES)//outputname,status='unknown',action='write')

      write(IOUT,*) 'Time step # ',it
      write(IOUT,*) 'Time: ',sngl(((it-1)*DT-t0)/60.d0),' minutes'
      write(IOUT,*)
      write(IOUT,*) 'Max norm displacement vector U in solid in all slices (m) = ',Usolidnorm_all
      write(IOUT,*) 'Max non-dimensional potential Ufluid in fluid in all slices = ',Ufluidnorm_all
      write(IOUT,*)

      write(IOUT,*) 'Elapsed time in seconds = ',tCPU
      write(IOUT,"(' Elapsed time in hh:mm:ss = ',i4,' h ',i2.2,' m ',i2.2,' s')") ihours,iminutes,iseconds
      write(IOUT,*) 'Mean elapsed time per time step in seconds = ',tCPU/dble(it)
      write(IOUT,*)

      write(IOUT,*) 'Time steps done = ',it,' out of ',NSTEP
      write(IOUT,*) 'Time steps remaining = ',NSTEP - it
      write(IOUT,*) 'Estimated remaining time in seconds = ',t_remain
      write(IOUT,"(' Estimated remaining time in hh:mm:ss = ',i4,' h ',i2.2,' m ',i2.2,' s')") &
               ihours_remain,iminutes_remain,iseconds_remain
      write(IOUT,*)

      write(IOUT,*) 'Estimated total run time in seconds = ',t_total
      write(IOUT,"(' Estimated total run time in hh:mm:ss = ',i4,' h ',i2.2,' m ',i2.2,' s')") &
               ihours_total,iminutes_total,iseconds_total
      write(IOUT,*) 'We have done ',sngl(100.d0*dble(it)/dble(NSTEP)),'% of that'
      write(IOUT,*)

      if(it < NSTEP) then

      write(IOUT,"(' The run will finish approximately on (in local time): ',a3,' ',a3,' ',i2.2,', ',i4.4,' ',i2.2,':',i2.2)") &
          weekday_name(day_of_week),month_name(mon),day,year,hr,minutes

! print date and time estimate of end of run in another country.
! For instance: the code runs at Caltech in California but the person
! running the code is connected remotely from France, which has 9 hours more
      if(ADD_TIME_ESTIMATE_ELSEWHERE .and. HOURS_TIME_DIFFERENCE * 60 + MINUTES_TIME_DIFFERENCE /= 0) then
        if(HOURS_TIME_DIFFERENCE * 60 + MINUTES_TIME_DIFFERENCE > 0) then
          write(IOUT,*) 'Adding positive time difference of ',abs(HOURS_TIME_DIFFERENCE),' hours'
        else
          write(IOUT,*) 'Adding negative time difference of ',abs(HOURS_TIME_DIFFERENCE),' hours'
        endif
        write(IOUT,*) 'and ',abs(MINUTES_TIME_DIFFERENCE),' minutes to get estimate at a remote location'
        write(IOUT, &
            "(' The run will finish approximately on (in remote time): ',a3,' ',a3,' ',i2.2,', ',i4.4,' ',i2.2,':',i2.2)") &
            weekday_name(day_of_week_remote),month_name(mon_remote),day_remote,year_remote,hr_remote,minutes_remote
      endif

      if(it < 100) then
        write(IOUT,*)
        write(IOUT,*) '************************************************************'
        write(IOUT,*) '**** BEWARE: the above time estimates are not reliable'
        write(IOUT,*) '**** because fewer than 100 iterations have been performed'
        write(IOUT,*) '************************************************************'
      endif

      endif

      close(IOUT)

! check stability of the code, exit if unstable
! negative values can occur with some compilers when the unstable value is greater
! than the greatest possible floating-point number of the machine
      if(Usolidnorm_all > STABILITY_THRESHOLD .or. Usolidnorm_all < 0) &
        call exit_MPI(myrank,'forward simulation became unstable and blew up in the solid')
      if(Ufluidnorm_all > STABILITY_THRESHOLD .or. Ufluidnorm_all < 0) &
        call exit_MPI(myrank,'forward simulation became unstable and blew up in the fluid')
    endif
  endif

! ****************************************************
!   big loop over all spectral elements in the fluid
! ****************************************************

! compute internal forces in the fluid region
  if(CUSTOM_REAL == SIZE_REAL) then
    time = sngl((dble(it-1)*DT-t0)/scale_t)
  else
    time = (dble(it-1)*DT-t0)/scale_t
  endif

! accel_outer_core, div_displ_outer_core are initialized to zero in the following subroutine.
  call compute_forces_outer_core(d_ln_density_dr_table, &
         displ_outer_core,accel_outer_core,xstore_outer_core,ystore_outer_core,zstore_outer_core, &
         xix_outer_core,xiy_outer_core,xiz_outer_core, &
         etax_outer_core,etay_outer_core,etaz_outer_core, &
         gammax_outer_core,gammay_outer_core,gammaz_outer_core, &
         hprime_xx,hprime_yy,hprime_zz,hprimewgll_xx,hprimewgll_yy,hprimewgll_zz, &
         wgllwgll_xy,wgllwgll_xz,wgllwgll_yz,ibool_outer_core)

! ****************************************************
! **********  add matching with solid part  **********
! ****************************************************

! only for elements in first matching layer in the fluid

!---
!--- couple with mantle at the top of the outer core
!---

  if(ACTUALLY_COUPLE_FLUID_CMB) then

! for surface elements exactly on the CMB
    do ispec2D = 1,NSPEC2D_TOP(IREGION_OUTER_CORE)
      ispec = ibelm_top_outer_core(ispec2D)

! only for DOFs exactly on the CMB (top of these elements)
      k = NGLLZ
      do j = 1,NGLLY
        do i = 1,NGLLX

! get velocity on the solid side using pointwise matching
          ispec_selected = ibelm_bottom_crust_mantle(ispec2D)

! corresponding points are located at the bottom of the mantle
          k_corresp = 1
          iglob = ibool_crust_mantle(i,j,k_corresp,ispec_selected)

          displ_x = displ_crust_mantle(1,iglob)
          displ_y = displ_crust_mantle(2,iglob)
          displ_z = displ_crust_mantle(3,iglob)

! get global point number
          iglob = ibool_outer_core(i,j,k,ispec)

! get normal on the CMB
          nx = normal_top_outer_core(1,i,j,ispec2D)
          ny = normal_top_outer_core(2,i,j,ispec2D)
          nz = normal_top_outer_core(3,i,j,ispec2D)

! compute dot product
          displ_n = displ_x*nx + displ_y*ny + displ_z*nz

! formulation with generalized potential
          weight = jacobian2D_top_outer_core(i,j,ispec2D)*wgllwgll_xy(i,j)

          accel_outer_core(iglob) = accel_outer_core(iglob) + weight*displ_n

        enddo
      enddo
    enddo

    endif

!---
!--- couple with inner core at the bottom of the outer core
!---

  if(ACTUALLY_COUPLE_FLUID_ICB .and. NCHUNKS_VAL == 6) then

! for surface elements exactly on the ICB
    do ispec2D = 1,NSPEC2D_BOTTOM(IREGION_OUTER_CORE)
      ispec = ibelm_bottom_outer_core(ispec2D)

! only for DOFs exactly on the ICB (bottom of these elements)
      k = 1
      do j = 1,NGLLY
        do i = 1,NGLLX

! get velocity on the solid side using pointwise matching
          ispec_selected = ibelm_top_inner_core(ispec2D)

! corresponding points are located at the bottom of the mantle
          k_corresp = NGLLZ
          iglob = ibool_inner_core(i,j,k_corresp,ispec_selected)

          displ_x = displ_inner_core(1,iglob)
          displ_y = displ_inner_core(2,iglob)
          displ_z = displ_inner_core(3,iglob)

! get global point number
          iglob = ibool_outer_core(i,j,k,ispec)

! get normal on the ICB
          nx = normal_bottom_outer_core(1,i,j,ispec2D)
          ny = normal_bottom_outer_core(2,i,j,ispec2D)
          nz = normal_bottom_outer_core(3,i,j,ispec2D)

! compute dot product
          displ_n = displ_x*nx + displ_y*ny + displ_z*nz

! formulation with generalized potential
          weight = jacobian2D_bottom_outer_core(i,j,ispec2D)*wgllwgll_xy(i,j)

          accel_outer_core(iglob) = accel_outer_core(iglob) - weight*displ_n

        enddo
      enddo
    enddo

  endif

! assemble all the contributions between slices using MPI

! outer core
  call assemble_MPI_scalar(myrank,accel_outer_core,NGLOB_OUTER_CORE, &
            iproc_xi,iproc_eta,ichunk,addressing, &
            iboolleft_xi_outer_core,iboolright_xi_outer_core,iboolleft_eta_outer_core,iboolright_eta_outer_core, &
            npoin2D_faces_outer_core,npoin2D_xi_outer_core,npoin2D_eta_outer_core, &
            iboolfaces_outer_core,iboolcorner_outer_core, &
            iprocfrom_faces,iprocto_faces,imsg_type, &
            iproc_master_corners,iproc_worker1_corners,iproc_worker2_corners, &
            buffer_send_faces,buffer_received_faces,npoin2D_max_all, &
            buffer_send_chunkcorners_scalar,buffer_recv_chunkcorners_scalar, &
            NUMMSGS_FACES,NUM_MSG_TYPES,NCORNERSCHUNKS, &
            NPROC_XI,NPROC_ETA,NGLOB1D_RADIAL(IREGION_OUTER_CORE), &
            NGLOB2DMAX_XMIN_XMAX(IREGION_OUTER_CORE),NGLOB2DMAX_YMIN_YMAX(IREGION_OUTER_CORE),NGLOB2DMAX_XY_VAL_OC,NCHUNKS)

! multiply by the inverse of the mass matrix and update velocity
  do i=1,NGLOB_OUTER_CORE
    accel_outer_core(i) = accel_outer_core(i)*rmass_outer_core(i)
    veloc_outer_core(i) = veloc_outer_core(i) + deltatover2*accel_outer_core(i)
  enddo

! ****************************************************
!   big loop over all spectral elements in the solid
! ****************************************************

! compute internal forces in the solid regions

! for anisotropy and gravity, x y and z contain r theta and phi

  call compute_forces_crust_mantle(displ_crust_mantle,accel_crust_mantle, &
          xstore_crust_mantle,ystore_crust_mantle,zstore_crust_mantle, &
          xix_crust_mantle,xiy_crust_mantle,xiz_crust_mantle, &
          etax_crust_mantle,etay_crust_mantle,etaz_crust_mantle, &
          gammax_crust_mantle,gammay_crust_mantle,gammaz_crust_mantle, &
          hprime_xx,hprime_yy,hprime_zz, &
          hprimewgll_xx,hprimewgll_yy,hprimewgll_zz, &
          wgllwgll_xy,wgllwgll_xz,wgllwgll_yz, &
          kappavstore_crust_mantle,kappahstore_crust_mantle,muvstore_crust_mantle, &
          muhstore_crust_mantle,eta_anisostore_crust_mantle, &
          ibool_crust_mantle,idoubling_crust_mantle, &
          R_memory_crust_mantle,epsilondev_crust_mantle,one_minus_sum_beta_crust_mantle, &
          alphaval,betaval,gammaval,factor_common_crust_mantle, &
          size(factor_common_crust_mantle,2), size(factor_common_crust_mantle,3), &
          size(factor_common_crust_mantle,4), size(factor_common_crust_mantle,5),COMPUTE_AND_STORE_STRAIN,AM_V)

  call compute_forces_inner_core(displ_inner_core,accel_inner_core,xstore_inner_core, &
          xix_inner_core,xiy_inner_core,xiz_inner_core, &
          etax_inner_core,etay_inner_core,etaz_inner_core, &
          gammax_inner_core,gammay_inner_core,gammaz_inner_core, &
          hprime_xx,hprime_yy,hprime_zz,hprimewgll_xx,hprimewgll_yy,hprimewgll_zz, &
          wgllwgll_xy,wgllwgll_xz,wgllwgll_yz, &
          kappavstore_inner_core,muvstore_inner_core,ibool_inner_core,idoubling_inner_core, &
          R_memory_inner_core,epsilondev_inner_core,one_minus_sum_beta_inner_core, &
          alphaval,betaval,gammaval, &
          factor_common_inner_core, &
          size(factor_common_inner_core,2), size(factor_common_inner_core,3), &
          size(factor_common_inner_core,4), size(factor_common_inner_core,5),COMPUTE_AND_STORE_STRAIN,AM_V)

! add the sources
  do isource = 1,NSOURCES

! add only if this proc carries the source
    if(myrank == islice_selected_source(isource)) then

      stf = comp_source_time_function(dble(it-1)*DT-t0-t_cmt(isource),hdur_gaussian(isource))

!     distinguish between single and double precision for reals
      if(CUSTOM_REAL == SIZE_REAL) then
        stf_used = sngl(stf)
      else
        stf_used = stf
      endif

!     add source array
      do k=1,NGLLZ
        do j=1,NGLLY
          do i=1,NGLLX
            iglob = ibool_crust_mantle(i,j,k,ispec_selected_source(isource))
            accel_crust_mantle(:,iglob) = accel_crust_mantle(:,iglob) + sourcearrays(isource,:,i,j,k)*stf_used
          enddo
        enddo
      enddo

    endif

  enddo

! ****************************************************
! **********  add matching with fluid part  **********
! ****************************************************

! only for elements in first matching layer in the solid

!---
!--- couple with outer core at the bottom of the mantle
!---

  if(ACTUALLY_COUPLE_FLUID_CMB) then

! for surface elements exactly on the CMB
    do ispec2D = 1,NSPEC2D_BOTTOM(IREGION_CRUST_MANTLE)

      ispec = ibelm_bottom_crust_mantle(ispec2D)

! only for DOFs exactly on the CMB (bottom of these elements)
      k = 1
      do j = 1,NGLLY
        do i = 1,NGLLX

! get velocity potential on the fluid side using pointwise matching
          ispec_selected = ibelm_top_outer_core(ispec2D)
          k_corresp = NGLLZ

! get normal at the CMB
          nx = normal_top_outer_core(1,i,j,ispec2D)
          ny = normal_top_outer_core(2,i,j,ispec2D)
          nz = normal_top_outer_core(3,i,j,ispec2D)

! get global point number
! corresponding points are located at the top of the outer core
          iglob = ibool_outer_core(i,j,NGLLZ,ispec_selected)
          iglob_mantle = ibool_crust_mantle(i,j,k,ispec)

! compute pressure, taking gravity into account
            pressure = - RHO_TOP_OC * accel_outer_core(iglob)

! formulation with generalized potential
          weight = jacobian2D_top_outer_core(i,j,ispec2D)*wgllwgll_xy(i,j)

          accel_crust_mantle(1,iglob_mantle) = accel_crust_mantle(1,iglob_mantle) + weight*nx*pressure
          accel_crust_mantle(2,iglob_mantle) = accel_crust_mantle(2,iglob_mantle) + weight*ny*pressure
          accel_crust_mantle(3,iglob_mantle) = accel_crust_mantle(3,iglob_mantle) + weight*nz*pressure

        enddo
      enddo
    enddo

  endif

!---
!--- couple with outer core at the top of the inner core
!---

  if(ACTUALLY_COUPLE_FLUID_ICB .and. NCHUNKS_VAL == 6) then

! for surface elements exactly on the ICB
    do ispec2D = 1,NSPEC2D_TOP(IREGION_INNER_CORE)

      ispec = ibelm_top_inner_core(ispec2D)

! only for DOFs exactly on the ICB (top of these elements)
      k = NGLLZ
      do j = 1,NGLLY
        do i = 1,NGLLX

! get velocity potential on the fluid side using pointwise matching
          ispec_selected = ibelm_bottom_outer_core(ispec2D)
          k_corresp = 1

! get normal at the ICB
          nx = normal_bottom_outer_core(1,i,j,ispec2D)
          ny = normal_bottom_outer_core(2,i,j,ispec2D)
          nz = normal_bottom_outer_core(3,i,j,ispec2D)

! get global point number
! corresponding points are located at the bottom of the outer core
          iglob = ibool_outer_core(i,j,k_corresp,ispec_selected)
          iglob_inner_core = ibool_inner_core(i,j,k,ispec)

! compute pressure, taking gravity into account
            pressure = - RHO_BOTTOM_OC * accel_outer_core(iglob)

! formulation with generalized potential
          weight = jacobian2D_bottom_outer_core(i,j,ispec2D)*wgllwgll_xy(i,j)

          accel_inner_core(1,iglob_inner_core) = accel_inner_core(1,iglob_inner_core) - weight*nx*pressure
          accel_inner_core(2,iglob_inner_core) = accel_inner_core(2,iglob_inner_core) - weight*ny*pressure
          accel_inner_core(3,iglob_inner_core) = accel_inner_core(3,iglob_inner_core) - weight*nz*pressure

        enddo
      enddo
    enddo

    endif

! assemble all the contributions between slices using MPI

! crust/mantle and inner core handled in the same call
! in order to reduce the number of MPI messages by 2
  call assemble_MPI_vector(myrank,accel_crust_mantle,accel_inner_core, &
            iproc_xi,iproc_eta,ichunk,addressing, &
            iboolleft_xi_crust_mantle,iboolright_xi_crust_mantle,iboolleft_eta_crust_mantle,iboolright_eta_crust_mantle, &
            npoin2D_faces_crust_mantle,npoin2D_xi_crust_mantle(1),npoin2D_eta_crust_mantle(1), &
            iboolfaces_crust_mantle,iboolcorner_crust_mantle, &
            iboolleft_xi_inner_core,iboolright_xi_inner_core,iboolleft_eta_inner_core,iboolright_eta_inner_core, &
            npoin2D_faces_inner_core,npoin2D_xi_inner_core(1),npoin2D_eta_inner_core(1), &
            iboolfaces_inner_core,iboolcorner_inner_core, &
            iprocfrom_faces,iprocto_faces,imsg_type, &
            iproc_master_corners,iproc_worker1_corners,iproc_worker2_corners, &
            buffer_send_faces,buffer_received_faces,npoin2D_max_all, &
            buffer_send_chunkcorners_vector,buffer_recv_chunkcorners_vector, &
            NUMMSGS_FACES,NUM_MSG_TYPES,NCORNERSCHUNKS, &
            NPROC_XI,NPROC_ETA,NGLOB1D_RADIAL(IREGION_CRUST_MANTLE), &
            NGLOB1D_RADIAL(IREGION_INNER_CORE),NCHUNKS,NDIM_smaller_buffers)

!---
!---  use buffers to assemble forces with the central cube
!---

  if(INCLUDE_CENTRAL_CUBE) then

   call assemble_MPI_central_cube(ichunk,nb_msgs_theor_in_cube, sender_from_slices_to_cube, &
     npoin2D_cube_from_slices, buffer_all_cube_from_slices, buffer_slices, buffer_slices2, ibool_central_cube, &
     receiver_cube_from_slices, ibool_inner_core, idoubling_inner_core, NSPEC_INNER_CORE, &
     ibelm_bottom_inner_core, NSPEC2D_BOTTOM(IREGION_INNER_CORE),NGLOB_INNER_CORE,accel_inner_core,NDIM)

  endif   ! end of assembling forces with the central cube

  do i=1,NGLOB_CRUST_MANTLE
    accel_crust_mantle(1,i) = accel_crust_mantle(1,i)*rmass_crust_mantle(i)
    accel_crust_mantle(2,i) = accel_crust_mantle(2,i)*rmass_crust_mantle(i)
    accel_crust_mantle(3,i) = accel_crust_mantle(3,i)*rmass_crust_mantle(i)
  enddo

  if(OCEANS) then

!   initialize the updates
    updated_dof_ocean_load(:) = .false.

! for surface elements exactly at the top of the crust (ocean bottom)
    do ispec2D = 1,NSPEC2D_TOP(IREGION_CRUST_MANTLE)

      ispec = ibelm_top_crust_mantle(ispec2D)

! only for DOFs exactly at the top of the crust (ocean bottom)
      k = NGLLZ

      do j = 1,NGLLY
        do i = 1,NGLLX

! get global point number
          iglob = ibool_crust_mantle(i,j,k,ispec)

! only update once
          if(.not. updated_dof_ocean_load(iglob)) then

! get normal
            nx = normal_top_crust_mantle(1,i,j,ispec2D)
            ny = normal_top_crust_mantle(2,i,j,ispec2D)
            nz = normal_top_crust_mantle(3,i,j,ispec2D)

! make updated component of right-hand side
! we divide by rmass_crust_mantle() which is 1 / M
! we use the total force which includes the Coriolis term above
            force_normal_comp = (accel_crust_mantle(1,iglob)*nx + &
                 accel_crust_mantle(2,iglob)*ny + &
                 accel_crust_mantle(3,iglob)*nz) / rmass_crust_mantle(iglob)

            additional_term = (rmass_ocean_load(iglob) - rmass_crust_mantle(iglob)) * force_normal_comp

            accel_crust_mantle(1,iglob) = accel_crust_mantle(1,iglob) + additional_term * nx
            accel_crust_mantle(2,iglob) = accel_crust_mantle(2,iglob) + additional_term * ny
            accel_crust_mantle(3,iglob) = accel_crust_mantle(3,iglob) + additional_term * nz

! done with this point
            updated_dof_ocean_load(iglob) = .true.

          endif

        enddo
      enddo
    enddo
  endif

  do i=1,NGLOB_CRUST_MANTLE
    veloc_crust_mantle(:,i) = veloc_crust_mantle(:,i) + deltatover2*accel_crust_mantle(:,i)
  enddo

  do i=1,NGLOB_INNER_CORE
    accel_inner_core(1,i) = accel_inner_core(1,i)*rmass_inner_core(i)
    accel_inner_core(2,i) = accel_inner_core(2,i)*rmass_inner_core(i)
    accel_inner_core(3,i) = accel_inner_core(3,i)*rmass_inner_core(i)

    veloc_inner_core(:,i) = veloc_inner_core(:,i) + deltatover2*accel_inner_core(:,i)
  enddo

! write the seismograms with time shift

! store the seismograms only if there is at least one receiver located in this slice
  if (nrec_local > 0) then

  do irec_local = 1,nrec_local

! get global number of that receiver
    irec = number_receiver_global(irec_local)

! perform the general interpolation using Lagrange polynomials
    uxd = ZERO
    uyd = ZERO
    uzd = ZERO

      do k = 1,NGLLZ
        do j = 1,NGLLY
          do i = 1,NGLLX

            iglob = ibool_crust_mantle(i,j,k,ispec_selected_rec(irec))

            hlagrange = hxir_store(irec_local,i)*hetar_store(irec_local,j)*hgammar_store(irec_local,k)

            uxd = uxd + dble(displ_crust_mantle(1,iglob))*hlagrange
            uyd = uyd + dble(displ_crust_mantle(2,iglob))*hlagrange
            uzd = uzd + dble(displ_crust_mantle(3,iglob))*hlagrange

          enddo
        enddo
      enddo
! store North, East and Vertical components

! distinguish between single and double precision for reals
      if(CUSTOM_REAL == SIZE_REAL) then
        seismograms(:,irec_local,seismo_current) = sngl(scale_displ*(nu(:,1,irec)*uxd + &
                   nu(:,2,irec)*uyd + nu(:,3,irec)*uzd))
      else
        seismograms(:,irec_local,seismo_current) = scale_displ*(nu(:,1,irec)*uxd + &
                   nu(:,2,irec)*uyd + nu(:,3,irec)*uzd)
      endif

    enddo

  endif ! nrec_local

! write the current or final seismograms
  if(seismo_current == NTSTEP_BETWEEN_OUTPUT_SEISMOS .or. it == it_end) then
      call write_seismograms(myrank,seismograms,number_receiver_global,station_name, &
            network_name,stlat,stlon,stele,nrec,nrec_local,DT,t0,it_end, &
            yr_SAC,jda_SAC,ho_SAC,mi_SAC,sec_SAC,t_cmt_SAC, &
            elat_SAC,elon_SAC,depth_SAC,mb_SAC,ename_SAC,cmt_lat_SAC,cmt_lon_SAC,&
            cmt_depth_SAC,cmt_hdur_SAC,NSOURCES_SAC,NPROCTOT, &
            OUTPUT_SEISMOS_ASCII_TEXT,OUTPUT_SEISMOS_SAC_ALPHANUM, &
            OUTPUT_SEISMOS_SAC_BINARY,ROTATE_SEISMOGRAMS_RT,NTSTEP_BETWEEN_OUTPUT_SEISMOS, &
            seismo_offset,seismo_current,WRITE_SEISMOGRAMS_BY_MASTER, &
            SAVE_ALL_SEISMOS_IN_ONE_FILE,USE_BINARY_FOR_LARGE_FILE,one_seismogram)
      if(myrank==0) then
        write(IMAIN,*)
        write(IMAIN,*) ' Total number of time steps written: ', it-it_begin+1
        write(IMAIN,*)
      endif
    seismo_offset = seismo_offset + seismo_current
    seismo_current = 0
  endif

!---- end of time iteration loop
!
  enddo   ! end of main time loop

! close the main output file
  if(myrank == 0) then
    write(IMAIN,*)
    write(IMAIN,*) 'End of the simulation'
    write(IMAIN,*)
    close(IMAIN)
  endif

  end subroutine specfem3D

